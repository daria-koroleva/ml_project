{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ae1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15172cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Phishing Dataset\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "RAW_DATA_PATH = Path('./data/raw/') \n",
    "PREPROCESSED_DATA_PATH = Path('data/preprocessing/') \n",
    "PREPROCESSED_DATA_PATH.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622c1758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV files:\n",
      "\n",
      "- CEAS_08.csv\n",
      "- Enron.csv\n",
      "- Ling.csv\n",
      "- Nazario.csv\n",
      "- Nigerian_Fraud.csv\n",
      "- SpamAssasin.csv\n"
     ]
    }
   ],
   "source": [
    "#Find all csv files\n",
    "csv_files = sorted(list(RAW_DATA_PATH.glob('*.csv')))\n",
    "print(f\"Found {len(csv_files)} CSV files:\\n\")\n",
    "for f in csv_files:\n",
    "    print(f\"- {f.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7946edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "File 1: CEAS_08.csv\n",
      "======================================================================\n",
      "Encoding: utf-8\n",
      "Shape: (39154, 7)\n",
      "Columns: ['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls']\n",
      "\n",
      "First 2 rows:\n",
      "                             sender                       receiver  \\\n",
      "0  Young Esposito <Young@iworld.de>    user4@gvc.ceas-challenge.cc   \n",
      "1      Mok <ipline's1983@icable.ph>  user2.2@gvc.ceas-challenge.cc   \n",
      "\n",
      "                              date                    subject  \\\n",
      "0  Tue, 05 Aug 2008 16:31:02 -0700  Never agree to be a loser   \n",
      "1  Tue, 05 Aug 2008 18:31:03 -0500     Befriend Jenna Jameson   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0  Buck up, your troubles caused by small dimension will soon be over!\\nBecome a lover no woman wil...   \n",
      "1               \\nUpgrade your sex and pleasures with these techniques http://www.brightmade.com\\n\\n\\n   \n",
      "\n",
      "   label  urls  \n",
      "0      1     1  \n",
      "1      1     1  \n",
      "\n",
      "Label distribution:\n",
      "0    17312\n",
      "1    21842\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "File 2: Enron.csv\n",
      "======================================================================\n",
      "Encoding: utf-8\n",
      "Shape: (29767, 3)\n",
      "Columns: ['subject', 'body', 'label']\n",
      "\n",
      "First 2 rows:\n",
      "                            subject  \\\n",
      "0         hpl nom for may 25 , 2001   \n",
      "1  re : nom / actual vols for 24 th   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0                                         ( see attached file : hplno 525 . xls )\\r\\n- hplno 525 . xls   \n",
      "1  - - - - - - - - - - - - - - - - - - - - - - forwarded by sabrae zajac / hou / ect on 05 / 30 / 2...   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "\n",
      "Label distribution:\n",
      "0    15791\n",
      "1    13976\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "File 3: Ling.csv\n",
      "======================================================================\n",
      "Encoding: utf-8\n",
      "Shape: (2859, 3)\n",
      "Columns: ['subject', 'body', 'label']\n",
      "\n",
      "First 2 rows:\n",
      "                                   subject  \\\n",
      "0  job posting - apple-iss research center   \n",
      "1                                      NaN   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0  content - length : 3386 apple-iss research center a us $ 10 million joint venture between apple ...   \n",
      "1  lang classification grimes , joseph e . and barbara f . grimes ; ethnologue language family inde...   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "\n",
      "Label distribution:\n",
      "0    2401\n",
      "1     458\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "File 4: Nazario.csv\n",
      "======================================================================\n",
      "Loaded data\\raw\\Nazario.csv normally.\n",
      "Encoding: utf-8\n",
      "Shape: (1565, 3)\n",
      "Columns: ['subject', 'body', 'label']\n",
      "\n",
      "First 2 rows:\n",
      "                                             subject  \\\n",
      "0  DON'T DELETE THIS MESSAGE -- FOLDER INTERNAL DATA   \n",
      "1                                Verify Your Account   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0  This text is part of the internal format of your mail folder, and is not a real message.  It is ...   \n",
      "1  Business with  \\t\\t\\t\\t\\t\\t\\t\\tcPanel & WHM \\t\\t\\t\\t\\t\\t\\t\\tDear client, Our Technical  \\t\\t\\t\\t...   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "\n",
      "Label distribution:\n",
      "1    1565\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "File 5: Nigerian_Fraud.csv\n",
      "======================================================================\n",
      "Encoding: utf-8\n",
      "Shape: (3332, 7)\n",
      "Columns: ['sender', 'receiver', 'date', 'subject', 'body', 'urls', 'label']\n",
      "\n",
      "First 2 rows:\n",
      "                                           sender              receiver  \\\n",
      "0  MR. JAMES NGOLA. <james_ngola2002@maktoob.com>  webmaster@aclweb.org   \n",
      "1  Mr. Ben Suleman <bensul2004nng@spinfinder.com>                   R@M   \n",
      "\n",
      "                              date  \\\n",
      "0  Thu, 31 Oct 2002 02:38:20 +0000   \n",
      "1  Thu, 31 Oct 2002 05:10:00 -0000   \n",
      "\n",
      "                                      subject  \\\n",
      "0  URGENT BUSINESS ASSISTANCE AND PARTNERSHIP   \n",
      "1         URGENT ASSISTANCE /RELATIONSHIP (P)   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\...   \n",
      "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Cu...   \n",
      "\n",
      "   urls  label  \n",
      "0     0      1  \n",
      "1     0      1  \n",
      "\n",
      "Label distribution:\n",
      "1    3332\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "File 6: SpamAssasin.csv\n",
      "======================================================================\n",
      "Encoding: utf-8\n",
      "Shape: (5809, 7)\n",
      "Columns: ['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls']\n",
      "\n",
      "First 2 rows:\n",
      "                                      sender  \\\n",
      "0             Robert Elz <kre@munnari.OZ.AU>   \n",
      "1  Steve Burt <Steve_Burt@cursor-system.com>   \n",
      "\n",
      "                                                     receiver  \\\n",
      "0  Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>   \n",
      "1   \"'zzzzteana@yahoogroups.com'\" <zzzzteana@yahoogroups.com>   \n",
      "\n",
      "                              date                    subject  \\\n",
      "0  Thu, 22 Aug 2002 18:26:25 +0700   Re: New Sequences Window   \n",
      "1  Thu, 22 Aug 2002 12:46:18 +0100  [zzzzteana] RE: Alexander   \n",
      "\n",
      "                                                                                                  body  \\\n",
      "0  Date:        Wed, 21 Aug 2002 10:54:46 -0500     From:        Chris Garrigues      Message-ID:  ...   \n",
      "1  Martin A posted:\\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\\n lim...   \n",
      "\n",
      "   label  urls  \n",
      "0      0     1  \n",
      "1      0     1  \n",
      "\n",
      "Label distribution:\n",
      "0    4091\n",
      "1    1718\n",
      "Name: label, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "Summary of all files:\n",
      "======================================================================\n",
      "             filename   rows  columns encoding\n",
      "0         CEAS_08.csv  39154        7    utf-8\n",
      "1           Enron.csv  29767        3    utf-8\n",
      "2            Ling.csv   2859        3    utf-8\n",
      "3         Nazario.csv   1565        3    utf-8\n",
      "4  Nigerian_Fraud.csv   3332        7    utf-8\n",
      "5     SpamAssasin.csv   5809        7    utf-8\n",
      "\n",
      "Total rows across all files: 82,486\n"
     ]
    }
   ],
   "source": [
    "# Helper to repair Nazario dataset\n",
    "def load_nazario_fixed(path):\n",
    "    \"\"\"Load and repair Nazario dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", error_bad_lines=False)\n",
    "        if {\"subject\",\"body\",\"label\"}.issubset(df.columns):\n",
    "            df = df[[\"subject\",\"body\",\"label\"]]\n",
    "            df[\"label\"] = 1\n",
    "            print(f\"Loaded {path} normally.\")\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"Normal CSV read failed: {e}\")\n",
    "\n",
    "    print(f\"Nazario file misaligned.Attempting manual repair for {path}\")\n",
    "    rows = []\n",
    "    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        m = re.search(r\",([01])\\s*$\", line)\n",
    "        if not m:\n",
    "            continue\n",
    "        label = int(m.group(1))\n",
    "        line = re.sub(r\",([01])\\s*$\", \"\", line)\n",
    "        parts = line.split(\",\", 4)\n",
    "        if len(parts) >= 4:\n",
    "            subj = parts[3].strip('\" ')\n",
    "            body = parts[4] if len(parts) > 4 else \"\"\n",
    "            rows.append({\"subject\": subj, \"body\": body, \"label\": 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"Nazario repaired: {len(df)} rows recovered.\")\n",
    "    return df\n",
    "\n",
    "#Load and Inspect each File\n",
    "raw_dataframes = []\n",
    "file_info = []\n",
    "\n",
    "for i, file_path in enumerate(csv_files, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"File {i}: {file_path.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    try:\n",
    "        if \"nazario\" in file_path.name.lower():\n",
    "            df = load_nazario_fixed(file_path)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', error_bad_lines=False)\n",
    "            encoding = 'utf-8'\n",
    "    except:\n",
    "            df = pd.read_csv(file_path, encoding='latin-1', error_bad_lines=False)\n",
    "            encoding = 'latin-1'\n",
    "    \n",
    "    print(f\"Encoding: {encoding}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 2 rows:\")\n",
    "    print(df.head(2))\n",
    "    \n",
    "    # Check for label column\n",
    "    if 'label' in df.columns or 'Label' in df.columns:\n",
    "        label_col = 'label' if 'label' in df.columns else 'Label'\n",
    "        print(f\"\\nLabel distribution:\")\n",
    "        print(df[label_col].value_counts().sort_index())\n",
    "    \n",
    "    raw_dataframes.append(df)\n",
    "    file_info.append({\n",
    "        'filename': file_path.name,\n",
    "        'rows': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'encoding': encoding\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Summary of all files:\")\n",
    "print(f\"{'='*70}\")\n",
    "summary_df = pd.DataFrame(file_info)\n",
    "print(summary_df)\n",
    "print(f\"\\nTotal rows across all files: {summary_df['rows'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Standardize Each File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c92058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataframe(df, file_name):\n",
    "\n",
    "    standardized = pd.DataFrame()\n",
    "    \n",
    "    # Extract subject\n",
    "    if 'subject' in df.columns:\n",
    "        standardized['subject'] = df['subject']\n",
    "    elif 'Subject' in df.columns:\n",
    "        standardized['subject'] = df['Subject']\n",
    "    else:\n",
    "        standardized['subject'] = ''\n",
    "        print(f\"{file_name}: No subject column\")\n",
    "    \n",
    "    # Extract body\n",
    "    if 'body' in df.columns:\n",
    "        standardized['body'] = df['body']\n",
    "    elif 'Body' in df.columns:\n",
    "        standardized['body'] = df['Body']\n",
    "    else:\n",
    "        standardized['body'] = ''\n",
    "        print(f\"{file_name}: No body column\")\n",
    "    \n",
    "    # Extract label\n",
    "    if 'label' in df.columns:\n",
    "        standardized['label'] = df['label']\n",
    "    elif 'Label' in df.columns:\n",
    "        standardized['label'] = df['Label']\n",
    "    else:\n",
    "        raise ValueError(f\"No label column found in {file_name}\")\n",
    "    \n",
    "    return standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde1fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEAS_08.csv\n",
      "Standardized: 39154 rows, 3 columns\n",
      "Enron.csv\n",
      "Standardized: 29767 rows, 3 columns\n",
      "Ling.csv\n",
      "Standardized: 2859 rows, 3 columns\n",
      "Nazario.csv\n",
      "Standardized: 1565 rows, 3 columns\n",
      "Nigerian_Fraud.csv\n",
      "Standardized: 3332 rows, 3 columns\n",
      "SpamAssasin.csv\n",
      "Standardized: 5809 rows, 3 columns\n",
      "\n",
      " Successfully standardized 6/6 files\n"
     ]
    }
   ],
   "source": [
    "standardized_dfs = []\n",
    "\n",
    "for df, file_info in zip(raw_dataframes, file_info):\n",
    "    file_name = file_info['filename']\n",
    "    print(f\"{file_name}\")\n",
    "    \n",
    "    try:\n",
    "        std_df = standardize_dataframe(df, file_name)\n",
    "        print(f\"Standardized: {len(std_df)} rows, 3 columns\")\n",
    "        standardized_dfs.append(std_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\n Successfully standardized {len(standardized_dfs)}/{len(raw_dataframes)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72956b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset:\n",
      "  Shape: (82486, 3)\n",
      "  Columns: ['subject', 'body', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all standardized dataframes\n",
    "merged_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Merged dataset:\")\n",
    "print(f\"  Shape: {merged_df.shape}\")\n",
    "print(f\"  Columns: {merged_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2caaf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: data\\preprocessing\\merged_raw.csv\n"
     ]
    }
   ],
   "source": [
    "merged_df.to_csv(PREPROCESSED_DATA_PATH / 'merged_raw.csv', index=False)\n",
    "print(f\" Saved: {PREPROCESSED_DATA_PATH / 'merged_raw.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ba1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 82,486 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = merged_df.copy()\n",
    "initial_count = len(df)\n",
    "print(f\"Before cleaning: {initial_count:,} rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing labels: 0\n",
      "Filled missing subject/body with empty strings\n",
      "\n",
      "Rows remaining: 82,486\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing labels\n",
    "missing_labels = df['label'].isna().sum()\n",
    "print(f\"Rows with missing labels: {missing_labels}\")\n",
    "if missing_labels > 0:\n",
    "    df = df.dropna(subset=['label'])\n",
    "    print(f\"Removed {missing_labels} rows\")\n",
    "\n",
    "# Fill missing subject/body with empty string\n",
    "df['subject'] = df['subject'].fillna('')\n",
    "df['body'] = df['body'].fillna('')\n",
    "print(f\"Filled missing subject/body with empty strings\")\n",
    "\n",
    "print(f\"\\nRows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af1357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels are valid (0 or 1)\n",
      "\n",
      "Rows remaining: 82,486\n"
     ]
    }
   ],
   "source": [
    "# Convert to string\n",
    "df['subject'] = df['subject'].astype(str)\n",
    "df['body'] = df['body'].astype(str)\n",
    "\n",
    "# Convert labels to int and validate\n",
    "df['label'] = df['label'].astype(int)\n",
    "unique_labels = sorted(df['label'].unique())\n",
    "\n",
    "# Check for invalid labels\n",
    "invalid = ~df['label'].isin([0, 1])\n",
    "if invalid.sum() > 0:\n",
    "    print(f\"Found {invalid.sum()} rows with invalid labels\")\n",
    "    print(f\"Invalid values: {df[invalid]['label'].unique()}\")\n",
    "    df = df[df['label'].isin([0, 1])]\n",
    "    print(f\"Removed {invalid.sum()} rows with invalid labels\")\n",
    "else:\n",
    "    print(\"All labels are valid (0 or 1)\")\n",
    "\n",
    "print(f\"\\nRows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54854ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning is done\n",
      "Created combined 'text' column to remove duplicates\n",
      "\n",
      "Rows remaining: 82,486\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str) or text == '' or pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Decode HTML entities (&lt; &gt; &amp; etc.)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # 2. Remove HTML tags \n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
    "    \n",
    "    # 3. Normalize Unicode characters \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # 4. Remove alternating junk patterns like >t=t=t=t= or +=+=+=+= (over 5)\n",
    "    text = re.sub(r'(.{1,3})\\1{5,}', '', text)\n",
    "    \n",
    "    # 5. Remove repeated junk characters: ===, ---, ___, +++ (over 3)\n",
    "    text = re.sub(r'([=\\-_+*#~|\\\\/<>])\\1{2,}', '', text)\n",
    "    \n",
    "    # 6. Remove repeated punctuation\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    text = re.sub(r'\\.{3,}', '...', text) \n",
    "    \n",
    "    # 7. Remove excessive letter/number repetitions\n",
    "    text = re.sub(r'(\\w)\\1{3,}', r'\\1\\1\\1', text)\n",
    "    \n",
    "    # 10. Collapse whitespace and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['subject'] = df['subject'].apply(clean_text)\n",
    "df['body'] = df['body'].apply(clean_text)\n",
    "print(\"Text cleaning is done\")\n",
    "\n",
    "# Create combined text column\n",
    "df['text'] = df['subject'] + ' ' + df['body']\n",
    "df['text'] = df['text'].str.strip()\n",
    "print(\"Created combined 'text' column to remove duplicates\")\n",
    "\n",
    "print(f\"\\nRows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty/invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with empty subject AND body: 0\n",
      "Rows with text less than 5 characters: 1\n",
      "Removed 1 rows\n",
      "\n",
      "Rows remaining: 82,485\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where both subject and body are empty\n",
    "empty_both = (df['subject'] == '') & (df['body'] == '')\n",
    "print(f\"Rows with empty subject AND body: {empty_both.sum()}\")\n",
    "if empty_both.sum() > 0:\n",
    "    df = df[~empty_both]\n",
    "    print(f\"Removed {empty_both.sum()} rows\")\n",
    "\n",
    "# Remove rows with very short text (< 5 characters)\n",
    "very_short = df['text'].str.len() < 5\n",
    "print(f\"Rows with text less than 5 characters: {very_short.sum()}\")\n",
    "if very_short.sum() > 0:\n",
    "    df = df[~very_short]\n",
    "    print(f\"Removed {very_short.sum()} rows\")\n",
    "\n",
    "print(f\"\\nRows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 592\n",
      "\n",
      "Example duplicates:\n",
      "'[UAI] Call for Papers: Special Issue M4M Call for Papers for \"Methods for Modalities\" A Special Issu...'\n",
      "\n",
      "Removed 592 duplicate rows\n",
      "\n",
      "Rows remaining: 81,893\n"
     ]
    }
   ],
   "source": [
    "# Find duplicates based on text\n",
    "duplicates = df.duplicated(subset=['text'], keep='first')\n",
    "print(f\"Duplicate rows: {duplicates.sum()}\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    # Show some examples\n",
    "    print(\"\\nExample duplicates:\")\n",
    "    dup_text = df[duplicates]['text'].iloc[0][:100]\n",
    "    print(f\"'{dup_text}...'\")\n",
    "    \n",
    "    df = df[~duplicates]\n",
    "    print(f\"\\nRemoved {duplicates.sum()} duplicate rows\")\n",
    "\n",
    "print(f\"\\nRows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Final Dataset and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (81893, 3)\n",
      "Columns: ['subject', 'body', 'label']\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimension will soon be over! Become a lover no woman will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>Upgrade your sex and pleasures with these techniques http://www.brightmade.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt; &gt;THE DAILY TOP 10 &gt;from CNN.com &gt;Top videos and stories as of: Aug 1, 2008 3:58 PM EDT &gt; TOP 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     subject  \\\n",
       "0  Never agree to be a loser   \n",
       "1     Befriend Jenna Jameson   \n",
       "2       CNN.com Daily Top 10   \n",
       "\n",
       "                                                                                                  body  \\\n",
       "0  Buck up, your troubles caused by small dimension will soon be over! Become a lover no woman will...   \n",
       "1                       Upgrade your sex and pleasures with these techniques http://www.brightmade.com   \n",
       "2  > >THE DAILY TOP 10 >from CNN.com >Top videos and stories as of: Aug 1, 2008 3:58 PM EDT > TOP 1...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create final dataset with all columns\n",
    "final_df = df[['subject', 'body', 'label']].copy()\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Columns: {final_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "display(final_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Summary:\n",
      "======================================================================\n",
      "\n",
      "Train set:\n",
      "Total:     57,325 (70.0%)\n",
      "Phishing:  29,841 (52.06%)\n",
      "Legitimate:27,484 (47.94%)\n",
      "\n",
      "Validation set:\n",
      "Total:     12,284 (15.0%)\n",
      "Phishing:  6,395 (52.06%)\n",
      "Legitimate:5,889 (47.94%)\n",
      "\n",
      "Test set:\n",
      "Total:     12,284 (15.0%)\n",
      "Phishing:  6,394 (52.05%)\n",
      "Legitimate:5,890 (47.95%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data first\n",
    "final_df = final_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = final_df[['subject', 'body']]\n",
    "y = final_df['label']\n",
    "\n",
    "# First split: 70% train, 30% temp(test+validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 15% val, 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "splits = [\n",
    "    ('Train', X_train, y_train),\n",
    "    ('Validation', X_val, y_val),\n",
    "    ('Test', X_test, y_test)\n",
    "]\n",
    "\n",
    "for name, X_split, y_split in splits:\n",
    "    total = len(y_split)\n",
    "    phishing = (y_split == 1).sum()\n",
    "    legit = (y_split == 0).sum()\n",
    "    pct = total / len(final_df) * 100\n",
    "    \n",
    "    print(f\"\\n{name} set:\")\n",
    "    print(f\"Total:     {total:,} ({pct:.1f}%)\")\n",
    "    print(f\"Phishing:  {phishing:,} ({phishing/total*100:.2f}%)\")\n",
    "    print(f\"Legitimate:{legit:,} ({legit/total*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\preprocessing\\phishing_full_clean.csv\n",
      "Saved: data\\preprocessing\\train.csv\n",
      "Saved: data\\preprocessing\\val.csv\n",
      "Saved: data\\preprocessing\\test.csv\n",
      "\n",
      "======================================================================\n",
      "All files saved successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "PREPROCESSED_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full dataset\n",
    "final_df.to_csv(PREPROCESSED_DATA_PATH / 'phishing_full_clean.csv', index=False)\n",
    "print(f\"Saved: {PREPROCESSED_DATA_PATH / 'phishing_full_clean.csv'}\")\n",
    "\n",
    "# Save train split\n",
    "train_df = X_train.copy()\n",
    "train_df['label'] = y_train.values\n",
    "train_df.to_csv(PREPROCESSED_DATA_PATH / 'train.csv', index=False)\n",
    "print(f\"Saved: {PREPROCESSED_DATA_PATH / 'train.csv'}\")\n",
    "\n",
    "# Save validation split\n",
    "val_df = X_val.copy()\n",
    "val_df['label'] = y_val.values\n",
    "val_df.to_csv(PREPROCESSED_DATA_PATH / 'val.csv', index=False)\n",
    "print(f\"Saved: {PREPROCESSED_DATA_PATH / 'val.csv'}\")\n",
    "\n",
    "# Save test split\n",
    "test_df = X_test.copy()\n",
    "test_df['label'] = y_test.values\n",
    "test_df.to_csv(PREPROCESSED_DATA_PATH / 'test.csv', index=False)\n",
    "print(f\"Saved: {PREPROCESSED_DATA_PATH / 'test.csv'}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All files saved successfully!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd6193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
