{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd4752b",
   "metadata": {},
   "source": [
    "# DistilBERT for Phishing Email Detection\n",
    "\n",
    "This notebook demonstrates how to build and evaluate a transformer-based model (DistilBERT) for phishing email classification with comprehensive evaluation metrics.\n",
    "\n",
    "## Overview\n",
    "- **Model**: DistilBERT for sequence classification\n",
    "- **Dataset**: Phishing vs legitimate emails  \n",
    "- **Optimization**: 4 lightweight training strategies (reduced length, mixed precision, pruning, TinyBERT)\n",
    "- **Evaluation**: Comprehensive metrics (accuracy, precision, recall, F1-score, PR-AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb60d2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25c038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME not installed. Install with: pip install lime\n",
      "SHAP not installed. Install with: pip install shap\n",
      "All libraries imported successfully!\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Sklearn for metrics and preprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Explainability libraries\n",
    "try:\n",
    "    import lime\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    print(\"LIME imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"LIME not installed. Install with: pip install lime\")\n",
    "    \n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80848c86-3c10-40a9-8120-37ce0c7177d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 2.7.1+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 10 Pro (10.0.19045 64-bit)\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.19045-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 9.0.176\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\n",
      "Nvidia driver version: 572.16\n",
      "cuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\cudnn_ops_train64_8.dll\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Name: 11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz\n",
      "Manufacturer: GenuineIntel\n",
      "Family: 1\n",
      "Architecture: 9\n",
      "ProcessorType: 3\n",
      "DeviceID: CPU0\n",
      "CurrentClockSpeed: 3504\n",
      "MaxClockSpeed: 3504\n",
      "L2CacheSize: 4096\n",
      "L2CacheSpeed: None\n",
      "Revision: None\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==2.2.6\n",
      "[pip3] torch==2.7.1+cu118\n",
      "[pip3] torchaudio==2.7.1+cu118\n",
      "[pip3] torchvision==0.22.1+cu118\n",
      "[conda] numpy                     2.2.6                    pypi_0    pypi\n",
      "[conda] torch                     2.7.1+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.7.1+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.22.1+cu118             pypi_0    pypi\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torch.utils.collect_env.main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c921a",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0bbf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from data/ directory\n",
      "\n",
      "Dataset Overview:\n",
      "Train samples: 35744\n",
      "Validation samples: 41702\n",
      "Test samples: 41702\n",
      "\n",
      "Dataset columns: ['text', 'type', 'source', 'word_count', 'sentence_count', 'words_per_sentence', 'domain', 'label']\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "Phishing      20373\n",
      "Legitimate    15371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Text length statistics:\n",
      "Mean: 1383 characters\n",
      "Median: 872 characters\n",
      "Max: 65535 characters\n",
      "\n",
      "Sample legitimate email:\n",
      "'How are you. Wish you a great semester...'\n",
      "\n",
      "Sample phishing email:\n",
      "'\n",
      "Attention: Sir/Madam,\n",
      "\n",
      "I will like to know if you can handle the supply of your product to African Union Development Authority, through the ongoing Supply Tender.\n",
      "\n",
      "The tender is open to all eligible ...'\n",
      "\n",
      "Data prepared for model training!\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/val.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Data loaded successfully from data/ directory\")\n",
    "\n",
    "SIMPLIFIED_TRAINING = False\n",
    "if SIMPLIFIED_TRAINING:\n",
    "    # keep only subset of samples for quicker experimentation\n",
    "    train_df = train_df.sample(n=500, random_state=42)\n",
    "    val_df = val_df.sample(n=100, random_state=42)\n",
    "    test_df = test_df.sample(n=100, random_state=42)\n",
    "\n",
    "# Data overview\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "print(f\"\\nDataset columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts().rename({0: 'Legitimate', 1: 'Phishing'}))\n",
    "\n",
    "# Text length analysis\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(f\"Mean: {train_df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Median: {train_df['text_length'].median():.0f} characters\")\n",
    "print(f\"Max: {train_df['text_length'].max()} characters\")\n",
    "\n",
    "# Show sample texts\n",
    "print(\"\\nSample legitimate email:\")\n",
    "legit_sample = train_df[train_df['label'] == 0]['text'].iloc[0]\n",
    "print(f\"'{legit_sample[:200]}...'\")\n",
    "\n",
    "print(\"\\nSample phishing email:\")\n",
    "phish_sample = train_df[train_df['label'] == 1]['text'].iloc[0]\n",
    "print(f\"'{phish_sample[:200]}...'\")\n",
    "\n",
    "# Prepare data for model training - use 'text' column directly\n",
    "X_train = train_df['text'].tolist()\n",
    "y_train = train_df['label'].tolist()\n",
    "X_val = val_df['text'].tolist()\n",
    "y_val = val_df['label'].tolist()\n",
    "X_test = test_df['text'].tolist()\n",
    "y_test = test_df['label'].tolist()\n",
    "\n",
    "print(f\"\\nData prepared for model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fabe5",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing for Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4976f",
   "metadata": {},
   "source": [
    "## 3.5 Optimization Options for Faster Training\n",
    "\n",
    "Choose ONE of the following lightweight approaches based on your needs:\n",
    "- **Option A**: DistilBERT with reduced sequence length (faster)\n",
    "- **Option B**: DistilBERT with mixed precision training (memory efficient)\n",
    "- **Option C**: DistilBERT with model pruning (smallest model)\n",
    "- **Option D**: TinyBERT (ultra-lightweight alternative)\n",
    "\n",
    "This section demonstrates all options. Uncomment your preferred approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5c3b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OPTIMIZATION SETUP\n",
      "============================================================\n",
      "\n",
      "Available optimization strategies:\n",
      "  1. Reduced Sequence Length (Fastest - minimal quality loss)\n",
      "  2. Mixed Precision Training (Fast - uses fp16)\n",
      "  3. Model Pruning (Medium - removes less important weights)\n",
      "  4. TinyBERT Model (Smallest - alternative ultra-light model)\n",
      "\n",
      " Selected: Option 4\n",
      "============================================================\n",
      "\n",
      "Option 4: TinyBERT Model\n",
      "------------------------------------------------------------\n",
      " Model: TinyBERT (4 layers, 312 hidden dims)\n",
      " Base model: huawei-noah/TinyBERT_General_4L_312D\n",
      " Max sequence length: 128\n",
      " Batch size: 64\n",
      " Epochs: 10\n",
      "\n",
      "  Expected speedup: ~10x faster than DistilBERT\n",
      "  Model size: Only 14MB vs DistilBERT 250MB\n",
      " Quality impact: Acceptable (2-5% accuracy drop)\n",
      "\n",
      "Configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZATION CONFIGURATION - preferred approach\n",
    "# ============================================================================\n",
    "\n",
    "# Set optimization preference here (1, 2, 3, or 4)\n",
    "OPTIMIZATION_CHOICE = 4  # 1=Reduced Length, 2=Mixed Precision, 3=Pruning, 4=TinyBERT\n",
    "\n",
    "print(\"MODEL OPTIMIZATION SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAvailable optimization strategies:\")\n",
    "print(\"  1. Reduced Sequence Length (Fastest - minimal quality loss)\")\n",
    "print(\"  2. Mixed Precision Training (Fast - uses fp16)\")\n",
    "print(\"  3. Model Pruning (Medium - removes less important weights)\")\n",
    "print(\"  4. TinyBERT Model (Smallest - alternative ultra-light model)\")\n",
    "print(f\"\\n Selected: Option {OPTIMIZATION_CHOICE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 1: REDUCED SEQUENCE LENGTH (RECOMMENDED FOR FASTEST TRAINING)\n",
    "# ============================================================================\n",
    "if OPTIMIZATION_CHOICE == 1:\n",
    "    print(\"\\n Option 1: Reduced Sequence Length\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Use shorter max length - much faster without much quality loss\n",
    "    MAX_LENGTH = 128  # Reduced from 512 (4x faster!)\n",
    "    BATCH_SIZE = 32   # Can use larger batches with shorter sequences\n",
    "    LEARNING_RATE = 3e-4  # Slightly higher LR for shorter sequences\n",
    "    NUM_EPOCHS = 2    # Reduced epochs to save time\n",
    "    \n",
    "    print(f\" Max sequence length: {MAX_LENGTH} (reduced from 512)\")\n",
    "    print(f\" Batch size: {BATCH_SIZE}\")\n",
    "    print(f\" Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\" Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"\\nExpected speedup: ~4-5x faster than full length\")\n",
    "    print(f\" Quality impact: Minimal (emails are usually concise)\")\n",
    "    \n",
    "    use_mixed_precision = False\n",
    "    use_pruning = False\n",
    "    tinybert = False\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 2: MIXED PRECISION TRAINING (fp16)\n",
    "# ============================================================================\n",
    "elif OPTIMIZATION_CHOICE == 2:\n",
    "    print(\"\\nOption 2: Mixed Precision Training (FP16)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    MAX_LENGTH = 256   # Medium length\n",
    "    BATCH_SIZE = 48    # Larger batch size possible with fp16\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 2\n",
    "    \n",
    "    print(f\" Max sequence length: {MAX_LENGTH}\")\n",
    "    print(f\" Batch size: {BATCH_SIZE}\")\n",
    "    print(f\" Precision: Mixed (FP32 + FP16)\")\n",
    "    print(f\" Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"\\nExpected speedup: ~2-3x faster + 50% less memory\")\n",
    "    print(f\"Quality impact: Negligible\")\n",
    "    \n",
    "    use_mixed_precision = True\n",
    "    use_pruning = False\n",
    "    tinybert = False\n",
    "    \n",
    "    # Install apex if using NVIDIA GPU for better mixed precision\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nFor best mixed precision performance on GPU:\")\n",
    "            print(\"   pip install apex\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 3: MODEL PRUNING (Remove 30% of less important weights)\n",
    "# ============================================================================\n",
    "elif OPTIMIZATION_CHOICE == 3:\n",
    "    print(\"\\n Option 3: Model Pruning\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 2\n",
    "    PRUNING_AMOUNT = 0.3  # Remove 30% of weights\n",
    "    \n",
    "    print(f\" Max sequence length: {MAX_LENGTH}\")\n",
    "    print(f\" Batch size: {BATCH_SIZE}\")\n",
    "    print(f\" Pruning amount: {PRUNING_AMOUNT*100:.0f}% of weights\")\n",
    "    print(f\" Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"\\nExpected speedup: ~2-3x faster + 30% smaller model\")\n",
    "    print(f\" Quality impact: Very small (1-2% accuracy drop)\")\n",
    "    \n",
    "    use_mixed_precision = False\n",
    "    use_pruning = True\n",
    "    tinybert = False\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 4: TinyBERT (Ultra-lightweight alternative)\n",
    "# ============================================================================\n",
    "elif OPTIMIZATION_CHOICE == 4:\n",
    "    print(\"\\nOption 4: TinyBERT Model\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    model_name = \"huawei-noah/TinyBERT_General_4L_312D\"  # 4 layers, 312 dims\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 64   # TinyBERT is so light you can use huge batches\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_EPOCHS = 10\n",
    "    \n",
    "    print(f\" Model: TinyBERT (4 layers, 312 hidden dims)\")\n",
    "    print(f\" Base model: {model_name}\")\n",
    "    print(f\" Max sequence length: {MAX_LENGTH}\")\n",
    "    print(f\" Batch size: {BATCH_SIZE}\")\n",
    "    print(f\" Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"\\n  Expected speedup: ~10x faster than DistilBERT\")\n",
    "    print(f\"  Model size: Only 14MB vs DistilBERT 250MB\")\n",
    "    print(f\" Quality impact: Acceptable (2-5% accuracy drop)\")\n",
    "    \n",
    "    use_mixed_precision = False\n",
    "    use_pruning = False\n",
    "    tinybert = True\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Using Option 1 (Reduced Sequence Length)\")\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_EPOCHS = 2\n",
    "    use_mixed_precision = False\n",
    "    use_pruning = False\n",
    "    tinybert = False\n",
    "\n",
    "print(\"\\nConfiguration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2aafd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      " Train dataset: 35744 samples\n",
      " Validation dataset: 41702 samples\n",
      " Test dataset: 41702 samples\n",
      "\n",
      "Tokenization example:\n",
      "Original text: '\n",
      "Attention: Sir/Madam,\n",
      "\n",
      "I will like to know if you can handle the supply of your product to African '\n",
      "Token IDs shape: torch.Size([1, 50])\n",
      "First 10 token IDs: [101, 3086, 1024, 2909, 1013, 21658, 1010, 1045, 2097, 2066]\n",
      "Decoded tokens: ['[CLS]', 'attention', ':', 'sir', '/', 'madam', ',', 'i', 'will', 'like']\n",
      "\n",
      "Token length statistics (sample):\n",
      "Mean tokens: 358.6\n",
      "95th percentile: 991\n",
      "Texts that would be truncated at 128: 670 (67.0%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize DistilBERT tokenizer\n",
    "if not tinybert:\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "# model_name already set if using TinyBERT option\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# MAX_LENGTH is now configured in optimization section above\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for email classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = EmailDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = EmailDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = EmailDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "print(f\" Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\" Validation dataset: {len(val_dataset)} samples\") \n",
    "print(f\" Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_text = X_train[0][:100]  # First 100 chars\n",
    "sample_encoding = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    max_length=min(50, MAX_LENGTH),  # Smaller for display\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization example:\")\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "print(f\"Token IDs shape: {sample_encoding['input_ids'].shape}\")\n",
    "print(f\"First 10 token IDs: {sample_encoding['input_ids'][0][:10].tolist()}\")\n",
    "print(f\"Decoded tokens: {tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][0][:10])}\")\n",
    "\n",
    "# Analyze text lengths after tokenization\n",
    "sample_lengths = []\n",
    "for text in X_train[:1000]:  # Sample first 1000 for speed\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors='pt')\n",
    "    sample_lengths.append(tokens['input_ids'].shape[1])\n",
    "\n",
    "print(f\"\\nToken length statistics (sample):\")\n",
    "print(f\"Mean tokens: {np.mean(sample_lengths):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(sample_lengths, 95):.0f}\")\n",
    "print(f\"Texts that would be truncated at {MAX_LENGTH}: {sum(1 for x in sample_lengths if x > MAX_LENGTH)} ({sum(1 for x in sample_lengths if x > MAX_LENGTH)/len(sample_lengths)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576b7162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model name: huawei-noah/TinyBERT_General_4L_312D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model loaded: DistilBertForSequenceClassification\n",
      " Number of parameters: 19,030,250\n",
      " Trainable parameters: 19,030,250\n",
      " Model size: 72.6 MB\n",
      "\n",
      " Trainer initialized successfully!\n",
      "\n",
      " Optimized Training Configuration:\n",
      "  - Model: huawei-noah/TinyBERT_General_4L_312D\n",
      "  - Epochs: 10\n",
      "  - Batch size: 64\n",
      "  - Max length: 128\n",
      "  - Learning rate: 0.0003\n",
      "  - Mixed precision: False\n",
      "  - Warmup steps: 50\n",
      "\n",
      " Training method: Hugging Face Trainer\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model based on optimization choice\n",
    "print(\"Loading model...\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification: phishing vs legitimate\n",
    "    output_attentions=True,  # Enable attention weights for explainability\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Apply model pruning if selected\n",
    "if use_pruning:\n",
    "    print(\"\\nApplying model pruning...\")\n",
    "    from torch.nn.utils.prune import global_unstructured, remove\n",
    "    \n",
    "    # Get all model parameters\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    if parameters_to_prune:\n",
    "        # Apply global pruning\n",
    "        global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=torch.nn.utils.prune.L1Unstructured,\n",
    "            amount=PRUNING_AMOUNT\n",
    "        )\n",
    "        \n",
    "        # Remove pruning reparameterization\n",
    "        for module, name in parameters_to_prune:\n",
    "            remove(module, name)\n",
    "        \n",
    "        print(f\" Pruned {PRUNING_AMOUNT*100:.0f}% of weights\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\n Model loaded: {model.__class__.__name__}\")\n",
    "print(f\" Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\" Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Calculate model size in MB\n",
    "model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024**2)\n",
    "print(f\" Model size: {model_size:.1f} MB\")\n",
    "\n",
    "# Try to use Trainer with optimizations\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    \n",
    "    # Define training arguments with optimizations\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./models/distilbert-phishing\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        warmup_steps=50,  # Reduced warmup\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,  # Evaluate less frequently\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=42,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        # Optimization flags\n",
    "        fp16=use_mixed_precision,  # Enable mixed precision\n",
    "        optim=\"adamw_torch\",  # Efficient optimizer\n",
    "        gradient_checkpointing=True,  # Trade compute for memory\n",
    "    )\n",
    "\n",
    "    # Define metrics computation function\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"\\n Trainer initialized successfully!\")\n",
    "    print(f\"\\n Optimized Training Configuration:\")\n",
    "    print(f\"  - Model: {model_name}\")\n",
    "    print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  - Max length: {MAX_LENGTH}\")\n",
    "    print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  - Mixed precision: {use_mixed_precision}\")\n",
    "    if use_pruning:\n",
    "        print(f\"  - Pruning: {PRUNING_AMOUNT*100:.0f}%\")\n",
    "    print(f\"  - Warmup steps: 50\")\n",
    "    \n",
    "    use_trainer = True\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\n Trainer import failed: {str(e)}\")\n",
    "    print(\" Will use manual training instead...\")\n",
    "    use_trainer = False\n",
    "\n",
    "print(f\"\\n Training method: {'Hugging Face Trainer' if use_trainer else 'Manual PyTorch'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f94aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accelerate version: 1.12.0\n",
      " Trainer imported successfully!\n",
      "Transformers version: 4.57.3\n",
      " All training components imported successfully!\n",
      "Ready to proceed with training!\n"
     ]
    }
   ],
   "source": [
    "# Check accelerate version for Trainer compatibility\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"Current accelerate version: {accelerate.__version__}\")\n",
    "    \n",
    "    # Try importing Trainer to see if it works\n",
    "    from transformers import Trainer\n",
    "    print(\" Trainer imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    if \"accelerate\" in str(e):\n",
    "        print(f\"Accelerate import error: {str(e)}\")\n",
    "        print(\"Trying to fix accelerate installation...\")\n",
    "        \n",
    "        # Try to install/upgrade accelerate\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"accelerate>=0.26.0\"])\n",
    "            print(\" Accelerate upgraded successfully!\")\n",
    "            \n",
    "            # Try importing again\n",
    "            import accelerate\n",
    "            from transformers import Trainer\n",
    "            print(f\" Now using accelerate version: {accelerate.__version__}\")\n",
    "            \n",
    "        except Exception as install_error:\n",
    "            print(f\"Failed to fix accelerate: {str(install_error)}\")\n",
    "            print(\"Manual fix needed: pip install --upgrade 'accelerate>=0.26.0'\")\n",
    "    else:\n",
    "        print(f\"Other import error: {str(e)}\")\n",
    "\n",
    "# Also check transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Try importing all required components\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    print(\" All training components imported successfully!\")\n",
    "    trainer_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"Training components import failed: {str(e)}\")\n",
    "    trainer_available = False\n",
    "\n",
    "if trainer_available:\n",
    "    print(\"Ready to proceed with training!\")\n",
    "else:\n",
    "    print(\"Training may not work. Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157c439",
   "metadata": {},
   "source": [
    "## 4. Create DistilBERT Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8c758",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c8d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model...\n",
      "Starting manual training...\n",
      "\n",
      "Epoch 1/10\n",
      "  Batch 20/559, Loss: 0.5701\n",
      "  Batch 40/559, Loss: 0.4246\n",
      "  Batch 60/559, Loss: 0.3550\n",
      "  Batch 80/559, Loss: 0.3028\n",
      "  Batch 100/559, Loss: 0.2697\n",
      "  Batch 120/559, Loss: 0.2436\n",
      "  Batch 140/559, Loss: 0.2271\n",
      "  Batch 160/559, Loss: 0.2104\n",
      "  Batch 180/559, Loss: 0.1991\n",
      "  Batch 200/559, Loss: 0.1892\n",
      "  Batch 220/559, Loss: 0.1824\n",
      "  Batch 240/559, Loss: 0.1788\n",
      "  Batch 260/559, Loss: 0.1723\n",
      "  Batch 280/559, Loss: 0.1661\n",
      "  Batch 300/559, Loss: 0.1620\n",
      "  Batch 320/559, Loss: 0.1580\n",
      "  Batch 340/559, Loss: 0.1531\n",
      "  Batch 360/559, Loss: 0.1491\n",
      "  Batch 380/559, Loss: 0.1465\n",
      "  Batch 400/559, Loss: 0.1439\n",
      "  Batch 420/559, Loss: 0.1403\n",
      "  Batch 440/559, Loss: 0.1370\n",
      "  Batch 460/559, Loss: 0.1343\n",
      "  Batch 480/559, Loss: 0.1312\n",
      "  Batch 500/559, Loss: 0.1293\n",
      "  Batch 520/559, Loss: 0.1277\n",
      "  Batch 540/559, Loss: 0.1259\n",
      "  Average Loss: 0.1233\n",
      "  Validation Loss: 0.0678\n",
      "  Validation Accuracy: 0.9767\n",
      "\n",
      "Epoch 2/10\n",
      "  Batch 20/559, Loss: 0.0311\n",
      "  Batch 40/559, Loss: 0.0328\n",
      "  Batch 60/559, Loss: 0.0342\n",
      "  Batch 80/559, Loss: 0.0346\n",
      "  Batch 100/559, Loss: 0.0337\n",
      "  Batch 120/559, Loss: 0.0349\n",
      "  Batch 140/559, Loss: 0.0364\n",
      "  Batch 160/559, Loss: 0.0385\n",
      "  Batch 180/559, Loss: 0.0378\n",
      "  Batch 200/559, Loss: 0.0386\n",
      "  Batch 220/559, Loss: 0.0385\n",
      "  Batch 240/559, Loss: 0.0400\n",
      "  Batch 260/559, Loss: 0.0425\n",
      "  Batch 280/559, Loss: 0.0410\n",
      "  Batch 300/559, Loss: 0.0424\n",
      "  Batch 320/559, Loss: 0.0430\n",
      "  Batch 340/559, Loss: 0.0425\n",
      "  Batch 360/559, Loss: 0.0430\n",
      "  Batch 380/559, Loss: 0.0440\n",
      "  Batch 400/559, Loss: 0.0444\n",
      "  Batch 420/559, Loss: 0.0452\n",
      "  Batch 440/559, Loss: 0.0466\n",
      "  Batch 460/559, Loss: 0.0468\n",
      "  Batch 480/559, Loss: 0.0466\n",
      "  Batch 500/559, Loss: 0.0459\n",
      "  Batch 520/559, Loss: 0.0454\n",
      "  Batch 540/559, Loss: 0.0446\n",
      "  Average Loss: 0.0441\n",
      "  Validation Loss: 0.0963\n",
      "  Validation Accuracy: 0.9789\n",
      "\n",
      "Epoch 3/10\n",
      "  Batch 20/559, Loss: 0.0508\n",
      "  Batch 40/559, Loss: 0.0384\n",
      "  Batch 60/559, Loss: 0.0406\n",
      "  Batch 80/559, Loss: 0.0381\n",
      "  Batch 100/559, Loss: 0.0401\n",
      "  Batch 120/559, Loss: 0.0375\n",
      "  Batch 140/559, Loss: 0.0374\n",
      "  Batch 160/559, Loss: 0.0353\n",
      "  Batch 180/559, Loss: 0.0351\n",
      "  Batch 200/559, Loss: 0.0369\n",
      "  Batch 220/559, Loss: 0.0382\n",
      "  Batch 240/559, Loss: 0.0372\n",
      "  Batch 260/559, Loss: 0.0381\n",
      "  Batch 280/559, Loss: 0.0389\n",
      "  Batch 300/559, Loss: 0.0384\n",
      "  Batch 320/559, Loss: 0.0391\n",
      "  Batch 340/559, Loss: 0.0393\n",
      "  Batch 360/559, Loss: 0.0390\n",
      "  Batch 380/559, Loss: 0.0399\n",
      "  Batch 400/559, Loss: 0.0400\n",
      "  Batch 420/559, Loss: 0.0401\n",
      "  Batch 440/559, Loss: 0.0406\n",
      "  Batch 460/559, Loss: 0.0415\n",
      "  Batch 480/559, Loss: 0.0415\n",
      "  Batch 500/559, Loss: 0.0411\n",
      "  Batch 520/559, Loss: 0.0409\n",
      "  Batch 540/559, Loss: 0.0409\n",
      "  Average Loss: 0.0407\n",
      "  Validation Loss: 0.0778\n",
      "  Validation Accuracy: 0.9826\n",
      "\n",
      "Epoch 4/10\n",
      "  Batch 20/559, Loss: 0.0219\n",
      "  Batch 40/559, Loss: 0.0349\n",
      "  Batch 60/559, Loss: 0.0315\n",
      "  Batch 80/559, Loss: 0.0319\n",
      "  Batch 100/559, Loss: 0.0291\n",
      "  Batch 120/559, Loss: 0.0257\n",
      "  Batch 140/559, Loss: 0.0266\n",
      "  Batch 160/559, Loss: 0.0273\n",
      "  Batch 180/559, Loss: 0.0262\n",
      "  Batch 200/559, Loss: 0.0271\n",
      "  Batch 220/559, Loss: 0.0268\n",
      "  Batch 240/559, Loss: 0.0276\n",
      "  Batch 260/559, Loss: 0.0284\n",
      "  Batch 280/559, Loss: 0.0304\n",
      "  Batch 300/559, Loss: 0.0316\n",
      "  Batch 320/559, Loss: 0.0328\n",
      "  Batch 340/559, Loss: 0.0324\n",
      "  Batch 360/559, Loss: 0.0327\n",
      "  Batch 380/559, Loss: 0.0337\n",
      "  Batch 400/559, Loss: 0.0333\n",
      "  Batch 420/559, Loss: 0.0343\n",
      "  Batch 440/559, Loss: 0.0346\n",
      "  Batch 460/559, Loss: 0.0353\n",
      "  Batch 480/559, Loss: 0.0348\n",
      "  Batch 500/559, Loss: 0.0357\n",
      "  Batch 520/559, Loss: 0.0359\n",
      "  Batch 540/559, Loss: 0.0358\n",
      "  Average Loss: 0.0358\n",
      "  Validation Loss: 0.0747\n",
      "  Validation Accuracy: 0.9780\n",
      "\n",
      "Epoch 5/10\n",
      "  Batch 20/559, Loss: 0.0389\n",
      "  Batch 40/559, Loss: 0.0286\n",
      "  Batch 60/559, Loss: 0.0251\n",
      "  Batch 80/559, Loss: 0.0285\n",
      "  Batch 100/559, Loss: 0.0290\n",
      "  Batch 120/559, Loss: 0.0299\n",
      "  Batch 140/559, Loss: 0.0301\n",
      "  Batch 160/559, Loss: 0.0307\n",
      "  Batch 180/559, Loss: 0.1252\n",
      "  Batch 200/559, Loss: 0.1788\n",
      "  Batch 220/559, Loss: 0.2209\n",
      "  Batch 240/559, Loss: 0.2551\n",
      "  Batch 260/559, Loss: 0.2843\n",
      "  Batch 280/559, Loss: 0.3097\n",
      "  Batch 300/559, Loss: 0.3302\n",
      "  Batch 320/559, Loss: 0.3487\n",
      "  Batch 340/559, Loss: 0.3652\n",
      "  Batch 360/559, Loss: 0.3802\n",
      "  Batch 380/559, Loss: 0.3924\n",
      "  Batch 400/559, Loss: 0.4042\n",
      "  Batch 420/559, Loss: 0.4143\n",
      "  Batch 440/559, Loss: 0.4239\n",
      "  Batch 460/559, Loss: 0.4326\n",
      "  Batch 480/559, Loss: 0.4404\n",
      "  Batch 500/559, Loss: 0.4481\n",
      "  Batch 520/559, Loss: 0.4551\n",
      "  Batch 540/559, Loss: 0.4610\n",
      "  Average Loss: 0.4668\n",
      "  Validation Loss: 0.6288\n",
      "  Validation Accuracy: 0.6345\n",
      "\n",
      "Epoch 6/10\n",
      "  Batch 20/559, Loss: 0.6270\n",
      "  Batch 40/559, Loss: 0.6223\n",
      "  Batch 60/559, Loss: 0.6242\n",
      "  Batch 80/559, Loss: 0.6268\n",
      "  Batch 100/559, Loss: 0.6268\n",
      "  Batch 120/559, Loss: 0.6286\n",
      "  Batch 140/559, Loss: 0.6306\n",
      "  Batch 160/559, Loss: 0.6311\n",
      "  Batch 180/559, Loss: 0.6319\n",
      "  Batch 200/559, Loss: 0.6325\n",
      "  Batch 220/559, Loss: 0.6321\n",
      "  Batch 240/559, Loss: 0.6331\n",
      "  Batch 260/559, Loss: 0.6341\n",
      "  Batch 280/559, Loss: 0.6342\n",
      "  Batch 300/559, Loss: 0.6344\n",
      "  Batch 320/559, Loss: 0.6345\n",
      "  Batch 340/559, Loss: 0.6342\n",
      "  Batch 360/559, Loss: 0.6344\n",
      "  Batch 380/559, Loss: 0.6342\n",
      "  Batch 400/559, Loss: 0.6343\n",
      "  Batch 420/559, Loss: 0.6341\n",
      "  Batch 440/559, Loss: 0.6347\n",
      "  Batch 460/559, Loss: 0.6349\n",
      "  Batch 480/559, Loss: 0.6350\n",
      "  Batch 500/559, Loss: 0.6349\n",
      "  Batch 520/559, Loss: 0.6345\n",
      "  Batch 540/559, Loss: 0.6346\n",
      "  Average Loss: 0.6347\n",
      "  Validation Loss: 0.6377\n",
      "  Validation Accuracy: 0.6248\n",
      "\n",
      "Epoch 7/10\n",
      "  Batch 20/559, Loss: 0.6390\n",
      "  Batch 40/559, Loss: 0.6434\n",
      "  Batch 60/559, Loss: 0.6419\n",
      "  Batch 80/559, Loss: 0.6401\n",
      "  Batch 100/559, Loss: 0.6401\n",
      "  Batch 120/559, Loss: 0.6396\n",
      "  Batch 140/559, Loss: 0.6380\n",
      "  Batch 160/559, Loss: 0.6374\n",
      "  Batch 180/559, Loss: 0.6372\n",
      "  Batch 200/559, Loss: 0.6370\n",
      "  Batch 220/559, Loss: 0.6352\n",
      "  Batch 240/559, Loss: 0.6338\n",
      "  Batch 260/559, Loss: 0.6337\n",
      "  Batch 280/559, Loss: 0.6331\n",
      "  Batch 300/559, Loss: 0.6329\n",
      "  Batch 320/559, Loss: 0.6331\n",
      "  Batch 340/559, Loss: 0.6331\n",
      "  Batch 360/559, Loss: 0.6330\n",
      "  Batch 380/559, Loss: 0.6326\n",
      "  Batch 400/559, Loss: 0.6327\n",
      "  Batch 420/559, Loss: 0.6323\n",
      "  Batch 440/559, Loss: 0.6325\n",
      "  Batch 460/559, Loss: 0.6321\n",
      "  Batch 480/559, Loss: 0.6318\n",
      "  Batch 500/559, Loss: 0.6317\n",
      "  Batch 520/559, Loss: 0.6314\n",
      "  Batch 540/559, Loss: 0.6310\n",
      "  Average Loss: 0.6310\n",
      "  Validation Loss: 0.6315\n",
      "  Validation Accuracy: 0.6344\n",
      "\n",
      "Epoch 8/10\n",
      "  Batch 20/559, Loss: 0.6230\n",
      "  Batch 40/559, Loss: 0.6211\n",
      "  Batch 60/559, Loss: 0.6229\n",
      "  Batch 80/559, Loss: 0.6257\n",
      "  Batch 100/559, Loss: 0.6260\n",
      "  Batch 120/559, Loss: 0.6269\n",
      "  Batch 140/559, Loss: 0.6273\n",
      "  Batch 160/559, Loss: 0.6275\n",
      "  Batch 180/559, Loss: 0.6264\n",
      "  Batch 200/559, Loss: 0.6247\n",
      "  Batch 220/559, Loss: 0.6251\n",
      "  Batch 240/559, Loss: 0.6258\n",
      "  Batch 260/559, Loss: 0.6264\n",
      "  Batch 280/559, Loss: 0.6325\n",
      "  Batch 300/559, Loss: 0.6321\n",
      "  Batch 320/559, Loss: 0.6315\n",
      "  Batch 340/559, Loss: 0.6293\n",
      "  Batch 360/559, Loss: 0.6124\n",
      "  Batch 380/559, Loss: 0.5985\n",
      "  Batch 400/559, Loss: 0.5832\n",
      "  Batch 420/559, Loss: 0.5706\n",
      "  Batch 440/559, Loss: 0.5606\n",
      "  Batch 460/559, Loss: 0.5483\n",
      "  Batch 480/559, Loss: 0.5416\n",
      "  Batch 500/559, Loss: 0.5321\n",
      "  Batch 520/559, Loss: 0.5219\n",
      "  Batch 540/559, Loss: 0.5144\n",
      "  Average Loss: 0.5164\n",
      "  Validation Loss: 0.2761\n",
      "  Validation Accuracy: 0.8968\n",
      "\n",
      "Epoch 9/10\n",
      "  Batch 20/559, Loss: 0.1509\n",
      "  Batch 40/559, Loss: 0.1805\n",
      "  Batch 60/559, Loss: 0.1799\n",
      "  Batch 80/559, Loss: 0.1877\n",
      "  Batch 100/559, Loss: 0.1707\n",
      "  Batch 120/559, Loss: 0.1664\n",
      "  Batch 140/559, Loss: 0.1597\n",
      "  Batch 160/559, Loss: 0.1566\n",
      "  Batch 180/559, Loss: 0.1579\n",
      "  Batch 200/559, Loss: 0.1570\n",
      "  Batch 220/559, Loss: 0.1506\n",
      "  Batch 240/559, Loss: 0.1463\n",
      "  Batch 260/559, Loss: 0.1417\n",
      "  Batch 280/559, Loss: 0.1371\n",
      "  Batch 300/559, Loss: 0.1332\n",
      "  Batch 320/559, Loss: 0.1285\n",
      "  Batch 340/559, Loss: 0.1282\n",
      "  Batch 360/559, Loss: 0.1251\n",
      "  Batch 380/559, Loss: 0.1224\n",
      "  Batch 400/559, Loss: 0.1189\n",
      "  Batch 420/559, Loss: 0.1170\n",
      "  Batch 440/559, Loss: 0.1137\n",
      "  Batch 460/559, Loss: 0.1234\n",
      "  Batch 480/559, Loss: 0.1216\n",
      "  Batch 500/559, Loss: 0.1201\n",
      "  Batch 520/559, Loss: 0.1197\n",
      "  Batch 540/559, Loss: 0.1198\n",
      "  Average Loss: 0.1191\n",
      "  Validation Loss: 0.1804\n",
      "  Validation Accuracy: 0.9542\n",
      "\n",
      "Epoch 10/10\n",
      "  Batch 20/559, Loss: 0.0734\n",
      "  Batch 40/559, Loss: 0.0784\n",
      "  Batch 60/559, Loss: 0.0822\n",
      "  Batch 80/559, Loss: 0.0852\n",
      "  Batch 100/559, Loss: 0.0894\n",
      "  Batch 120/559, Loss: 0.0920\n",
      "  Batch 140/559, Loss: 0.0854\n",
      "  Batch 160/559, Loss: 0.0943\n",
      "  Batch 180/559, Loss: 0.1267\n",
      "  Batch 200/559, Loss: 0.1577\n",
      "  Batch 220/559, Loss: 0.1623\n",
      "  Batch 240/559, Loss: 0.1613\n",
      "  Batch 260/559, Loss: 0.1652\n",
      "  Batch 280/559, Loss: 0.1640\n",
      "  Batch 300/559, Loss: 0.1647\n",
      "  Batch 320/559, Loss: 0.1623\n",
      "  Batch 340/559, Loss: 0.1592\n",
      "  Batch 360/559, Loss: 0.1588\n",
      "  Batch 380/559, Loss: 0.1591\n",
      "  Batch 400/559, Loss: 0.1590\n",
      "  Batch 420/559, Loss: 0.1590\n",
      "  Batch 440/559, Loss: 0.1591\n",
      "  Batch 460/559, Loss: 0.1602\n",
      "  Batch 480/559, Loss: 0.1601\n",
      "  Batch 500/559, Loss: 0.1603\n",
      "  Batch 520/559, Loss: 0.1581\n",
      "  Batch 540/559, Loss: 0.1557\n",
      "  Average Loss: 0.1553\n",
      "  Validation Loss: 0.1482\n",
      "  Validation Accuracy: 0.9465\n",
      "\n",
      "âœ“ Training completed! Model saved to models\\distilbert-phishing\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "Model: DistilBertForSequenceClassification\n",
      "Location: models\\distilbert-phishing\n",
      "Max sequence length: 128\n",
      "Ready for evaluation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check if pre-trained model exists\n",
    "model_save_path = Path(\"./models/distilbert-phishing\")\n",
    "model_exists = model_save_path.exists() and any(model_save_path.glob(\"*.bin\")) or any(model_save_path.glob(\"*.safetensors\"))\n",
    "use_trainer = False\n",
    "if model_exists:\n",
    "    print(\"Loading pre-trained model...\")\n",
    "    try:\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_save_path,\n",
    "            output_attentions=True\n",
    "        ).to(device)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_save_path)\n",
    "        print(\"Pre-trained model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\")\n",
    "        print(\"Will train a new model instead...\")\n",
    "        model_exists = False\n",
    "\n",
    "if not model_exists:\n",
    "    print(\"Training new model...\")\n",
    "    \n",
    "    if use_trainer:\n",
    "        # Use Hugging Face Trainer\n",
    "        try:\n",
    "            print(\"\\nTraining started...\")\n",
    "            trainer.train()\n",
    "            print(\"Training completed successfully!\")\n",
    "            \n",
    "            # Save the best model\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "            print(f\"Model saved to {training_args.output_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trainer failed: {str(e)}\")\n",
    "            print(\"Switching to manual training...\")\n",
    "            use_trainer = False\n",
    "    \n",
    "    if not use_trainer:\n",
    "        # Manual training loop with optimizations\n",
    "        print(\"Starting manual training...\")\n",
    "        \n",
    "        from torch.optim import AdamW\n",
    "        from torch.utils.data import DataLoader\n",
    "        \n",
    "        # Create data loaders with optimized batch size\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Setup optimizer with optimized learning rate\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Print progress\n",
    "                if batch_idx % 20 == 0 and batch_idx > 0:\n",
    "                    avg_loss = total_loss / (batch_idx + 1)\n",
    "                    print(f\"  Batch {batch_idx}/{len(train_dataloader)}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Validation every epoch\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, \n",
    "                                  attention_mask=attention_mask, \n",
    "                                  labels=labels)\n",
    "                    val_loss += outputs.loss.item()\n",
    "                    \n",
    "                    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                    val_correct += (predictions == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "            \n",
    "            val_accuracy = val_correct / val_total\n",
    "            print(f\"  Validation Loss: {val_loss/len(val_dataloader):.4f}\")\n",
    "            print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Save manually trained model\n",
    "        model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"\\nâœ“ Training completed! Model saved to {model_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Location: {model_save_path}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Ready for evaluation\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07490c4b",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dc0e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Accuracy:  0.9395\n",
      "Precision: 0.9430\n",
      "Recall:    0.9395\n",
      "F1-Score:  0.9389\n",
      "PR-AUC:    0.9888\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.98      0.87      0.93     17932\n",
      "    Phishing       0.91      0.99      0.95     23770\n",
      "\n",
      "    accuracy                           0.94     41702\n",
      "   macro avg       0.95      0.93      0.94     41702\n",
      "weighted avg       0.94      0.94      0.94     41702\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15652  2280]\n",
      " [  244 23526]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation pipeline with proper max_length handling\n",
    "class MaxLengthPipeline:\n",
    "    \"\"\"Wrapper to handle max_length in pipeline\"\"\"\n",
    "    def __init__(self, base_pipeline, max_length):\n",
    "        self.pipe = base_pipeline\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, texts, batch_size=None):\n",
    "        \"\"\"Handle both single texts and batches\"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Truncate texts to avoid sequence length mismatch\n",
    "        # Rough estimate: ~4 characters per token\n",
    "        max_chars = self.max_length * 4\n",
    "        truncated_texts = []\n",
    "        for text in texts:\n",
    "            if len(text) > max_chars:\n",
    "                text = text[:max_chars]\n",
    "            truncated_texts.append(text)\n",
    "        \n",
    "        # Call the underlying pipeline\n",
    "        return self.pipe(truncated_texts, batch_size=batch_size)\n",
    "\n",
    "# Create base pipeline\n",
    "base_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Wrap it with max_length handling\n",
    "eval_pipeline = MaxLengthPipeline(base_pipeline, MAX_LENGTH)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_predictions = []\n",
    "test_probabilities = []\n",
    "\n",
    "# Batch prediction for efficiency\n",
    "batch_size = 32\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    batch_texts = X_test[i:i + batch_size]\n",
    "    batch_results = eval_pipeline(batch_texts)\n",
    "    \n",
    "    for result in batch_results:\n",
    "        # Handle both nested list and flat list formats\n",
    "        if isinstance(result, list):  # Nested list: [[{...}, {...}], ...]\n",
    "            # Extract the scores from the nested list\n",
    "            scores = {r['label']: r['score'] for r in result}\n",
    "            phishing_score = scores.get('LABEL_1', 0.0)\n",
    "        else:  # Single dict: {...}\n",
    "            phishing_score = result['score'] if result['label'] == 'LABEL_1' else 1 - result['score']\n",
    "        \n",
    "        prediction = 1 if phishing_score > 0.5 else 0\n",
    "        test_predictions.append(prediction)\n",
    "        test_probabilities.append(phishing_score)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_pred = np.array(test_predictions)\n",
    "y_prob = np.array(test_probabilities)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Phishing']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Legitimate', 'Phishing'],\n",
    "           yticklabels=['Legitimate', 'Phishing'],\n",
    "           ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)\n",
    "axes[1].plot(recall_curve, precision_curve, marker='.', linewidth=2, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15c2ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and results...\n",
      "Model already exists at models\\distilbert-phishing\n",
      "âœ“ Results saved to results\\model_results.json\n",
      "âœ“ Predictions saved to results\\predictions.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "âœ“ Model trained: huawei-noah/TinyBERT_General_4L_312D\n",
      "âœ“ Optimization: Option 4\n",
      "âœ“ Max sequence length: 128\n",
      "âœ“ Test accuracy: 0.9395\n",
      "âœ“ Test F1-Score: 0.9389\n",
      "\n",
      "Files saved:\n",
      "  - Model: models\\distilbert-phishing\n",
      "  - Results: results\\model_results.json\n",
      "  - Predictions: results\\predictions.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save Model and Results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Saving model and results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model_dir = Path(\"models/distilbert-phishing\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "else:\n",
    "    print(f\"Model already exists at {model_dir}\")\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    \"model_info\": {\n",
    "        \"model_name\": model_name,\n",
    "        \"architecture\": \"DistilBERT for sequence classification\",\n",
    "        \"num_parameters\": int(sum(p.numel() for p in model.parameters())),\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"training_date\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(X_train),\n",
    "        \"val_size\": len(X_val), \n",
    "        \"test_size\": len(X_test),\n",
    "        \"class_distribution\": {\n",
    "            \"legitimate\": int(sum(1 for x in y_train if x == 0)),\n",
    "            \"phishing\": int(sum(1 for x in y_train if x == 1))\n",
    "        }\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_precision\": float(precision),\n",
    "        \"test_recall\": float(recall),\n",
    "        \"test_f1_score\": float(f1),\n",
    "        \"test_pr_auc\": float(pr_auc)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = results_dir / \"model_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"âœ“ Results saved to {results_file}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'phishing_probability': y_prob,\n",
    "    'correct_prediction': y_test == y_pred\n",
    "})\n",
    "\n",
    "predictions_file = results_dir / \"predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"âœ“ Predictions saved to {predictions_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Model trained: {model_name}\")\n",
    "print(f\"âœ“ Optimization: Option {OPTIMIZATION_CHOICE}\")\n",
    "print(f\"âœ“ Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"âœ“ Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ“ Test F1-Score: {f1:.4f}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - Model: {model_dir}\")\n",
    "print(f\"  - Results: {results_file}\")\n",
    "print(f\"  - Predictions: {predictions_file}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abec6c3",
   "metadata": {},
   "source": [
    "## 10. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fab49b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving model and results...\n",
      " Model already exists at models\\distilbert-phishing\n",
      "Results saved to results\\distilbert_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results\\distilbert_predictions.csv\n",
      "Inference function saved to results\\classify_email_function.pkl\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE!\n",
      "============================================================\n",
      "Model Performance Summary:\n",
      "Accuracy: 0.9395\n",
      "F1-Score: 0.9389\n",
      "PR-AUC: 0.9888\n",
      "\n",
      "Saved Files:\n",
      "Model: models\\distilbert-phishing\n",
      "Results: results\\distilbert_results.json\n",
      "Predictions: results\\distilbert_predictions.csv\n",
      "\n",
      "DistilBERT phishing detection model is ready!\n",
      "============================================================\n",
      "\n",
      "Testing saved model on sample phishing email:\n",
      "Sample: 'Get 50% off PS5! Click here now for amazing deals!'\n",
      "Prediction: Legitimate (confidence: 0.997)\n"
     ]
    }
   ],
   "source": [
    "# Save model, tokenizer, and results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\" Saving model and results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer (if not already saved)\n",
    "model_dir = Path(\"models/distilbert-phishing\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    print(f\" Model saved to {model_dir}\")\n",
    "else:\n",
    "    print(f\" Model already exists at {model_dir}\")\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    \"model_info\": {\n",
    "        \"model_name\": model_name,\n",
    "        \"architecture\": \"DistilBERT for sequence classification\",\n",
    "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"training_date\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(X_train),\n",
    "        \"val_size\": len(X_val), \n",
    "        \"test_size\": len(X_test),\n",
    "        \"class_distribution\": {\n",
    "            \"legitimate\": int(sum(1 for x in y_train if x == 0)),\n",
    "            \"phishing\": int(sum(1 for x in y_train if x == 1))\n",
    "        }\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_precision\": float(precision),\n",
    "        \"test_recall\": float(recall),\n",
    "        \"test_f1_score\": float(f1),\n",
    "        \"test_pr_auc\": float(pr_auc)\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_steps\": 50,\n",
    "        \"weight_decay\": 0.01\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = results_dir / \"distilbert_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'phishing_probability': y_prob,\n",
    "    'correct_prediction': y_test == y_pred\n",
    "})\n",
    "\n",
    "predictions_file = results_dir / \"distilbert_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Predictions saved to {predictions_file}\")\n",
    "\n",
    "# Create a simple inference function for future use\n",
    "def classify_email(text, model_path=\"models/distilbert-phishing\"):\n",
    "    \"\"\"\n",
    "    Classify a single email text using the trained DistilBERT model\n",
    "    \n",
    "    Args:\n",
    "        text (str): Email text to classify\n",
    "        model_path (str): Path to the saved model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Classification results with prediction and confidence\n",
    "    \"\"\"\n",
    "    # Create pipeline with max_length handling\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    results = classifier(text)\n",
    "    \n",
    "    # Handle both nested list and flat list formats\n",
    "    if isinstance(results[0], list):\n",
    "        # Nested list format: [[{...}, {...}]]\n",
    "        scores = {r['label']: r['score'] for r in results[0]}\n",
    "        phishing_score = scores.get('LABEL_1', 0.0)\n",
    "    else:\n",
    "        # Flat list format: [{...}, {...}]\n",
    "        scores = {r['label']: r['score'] for r in results}\n",
    "        phishing_score = scores.get('LABEL_1', 0.0)\n",
    "    \n",
    "    prediction = \"Phishing\" if phishing_score > 0.5 else \"Legitimate\"\n",
    "    confidence = phishing_score if prediction == \"Phishing\" else 1 - phishing_score\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"confidence\": confidence,\n",
    "        \"phishing_probability\": phishing_score,\n",
    "        \"legitimate_probability\": 1 - phishing_score\n",
    "    }\n",
    "\n",
    "# Save the inference function\n",
    "import pickle\n",
    "with open(results_dir / \"classify_email_function.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classify_email, f)\n",
    "\n",
    "print(f\"Inference function saved to {results_dir / 'classify_email_function.pkl'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model Performance Summary:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"\\nSaved Files:\")\n",
    "print(f\"Model: {model_dir}\")\n",
    "print(f\"Results: {results_file}\")\n",
    "print(f\"Predictions: {predictions_file}\")\n",
    "print(f\"\\nDistilBERT phishing detection model is ready!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example usage of the saved model\n",
    "print(f\"\\nTesting saved model on sample phishing email:\")\n",
    "sample_text = \"Get 50% off PS5! Click here now for amazing deals!\"\n",
    "result = classify_email(sample_text)\n",
    "print(f\"Sample: '{sample_text}'\")\n",
    "print(f\"Prediction: {result['prediction']} (confidence: {result['confidence']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b71491-5395-4cf9-863e-84f48ed7557e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-torch]",
   "language": "python",
   "name": "conda-env-conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
