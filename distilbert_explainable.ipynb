{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd4752b",
   "metadata": {},
   "source": [
    "# Explainable DistilBERT for Phishing Email Detection\n",
    "\n",
    "This notebook demonstrates how to build and explain a transformer-based model (DistilBERT) for phishing email classification with comprehensive explainability analysis using LIME and SHAP.\n",
    "\n",
    "## Overview\n",
    "- **Model**: DistilBERT for sequence classification\n",
    "- **Dataset**: Phishing vs legitimate emails  \n",
    "- **Explainability**: LIME and SHAP for model interpretability\n",
    "- **Evaluation**: Comprehensive metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb60d2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25c038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME not installed. Install with: pip install lime\n",
      "SHAP not installed. Install with: pip install shap\n",
      "All libraries imported successfully!\n",
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Sklearn for metrics and preprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Explainability libraries\n",
    "try:\n",
    "    import lime\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    print(\"LIME imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"LIME not installed. Install with: pip install lime\")\n",
    "    \n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c921a",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0bbf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully from existing splits\n",
      "\n",
      "üìä Dataset Overview:\n",
      "Train samples: 57325\n",
      "Validation samples: 12284\n",
      "Test samples: 12284\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "Phishing      29841\n",
      "Legitimate    27484\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Text length statistics:\n",
      "Mean: 1738 characters\n",
      "Median: 733 characters\n",
      "Max: 4546559 characters\n",
      "\n",
      "üìù Sample legitimate email:\n",
      "'Re: [Python-3000] Types and classes On Wed, Apr 2, 2008 at 5:51 PM, Guido van Rossum wrote: > I have no idea what you are saying here (and I did s/since/sense/ :-). Another lesson to me, that I should...'\n",
      "\n",
      "üé£ Sample phishing email:\n",
      "'Engaging RX Offers Superior Medical Reductions http://smallworldtho.spaces.live.com/default.aspx environment. In other In a way that makes you Linda F., New York...'\n",
      "\n",
      "‚úì Data prepared for model training!\n",
      "\n",
      "üìä Dataset Overview:\n",
      "Train samples: 57325\n",
      "Validation samples: 12284\n",
      "Test samples: 12284\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "Phishing      29841\n",
      "Legitimate    27484\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Text length statistics:\n",
      "Mean: 1738 characters\n",
      "Median: 733 characters\n",
      "Max: 4546559 characters\n",
      "\n",
      "üìù Sample legitimate email:\n",
      "'Re: [Python-3000] Types and classes On Wed, Apr 2, 2008 at 5:51 PM, Guido van Rossum wrote: > I have no idea what you are saying here (and I did s/since/sense/ :-). Another lesson to me, that I should...'\n",
      "\n",
      "üé£ Sample phishing email:\n",
      "'Engaging RX Offers Superior Medical Reductions http://smallworldtho.spaces.live.com/default.aspx environment. In other In a way that makes you Linda F., New York...'\n",
      "\n",
      "‚úì Data prepared for model training!\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"data/preprocessing/train.csv\")\n",
    "    val_df = pd.read_csv(\"data/preprocessing/val.csv\")\n",
    "    test_df = pd.read_csv(\"data/preprocessing/test.csv\")\n",
    "    print(\"‚úì Data loaded successfully from existing splits\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed data not found. Loading raw data...\")\n",
    "    # Add code here to load and preprocess raw data if needed\n",
    "\n",
    "# Combine subject and body text\n",
    "def combine_text(row):\n",
    "    subject = str(row['subject']) if pd.notna(row['subject']) else \"\"\n",
    "    body = str(row['body']) if pd.notna(row['body']) else \"\"\n",
    "    return f\"{subject} {body}\".strip()\n",
    "\n",
    "# Apply text combination to all datasets\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['text'] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# Data overview\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts().rename({0: 'Legitimate', 1: 'Phishing'}))\n",
    "\n",
    "# Text length analysis\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(f\"Mean: {train_df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Median: {train_df['text_length'].median():.0f} characters\")\n",
    "print(f\"Max: {train_df['text_length'].max()} characters\")\n",
    "\n",
    "# Show sample texts\n",
    "print(\"\\nüìù Sample legitimate email:\")\n",
    "legit_sample = train_df[train_df['label'] == 0]['text'].iloc[0]\n",
    "print(f\"'{legit_sample[:200]}...'\")\n",
    "\n",
    "print(\"\\nüé£ Sample phishing email:\")\n",
    "phish_sample = train_df[train_df['label'] == 1]['text'].iloc[0]\n",
    "print(f\"'{phish_sample[:200]}...'\")\n",
    "\n",
    "# Prepare data for model training\n",
    "X_train = train_df['text'].tolist()\n",
    "y_train = train_df['label'].tolist()\n",
    "X_val = val_df['text'].tolist()\n",
    "y_val = val_df['label'].tolist()\n",
    "X_test = test_df['text'].tolist()\n",
    "y_test = test_df['label'].tolist()\n",
    "\n",
    "print(f\"\\n‚úì Data prepared for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fabe5",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2aafd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1671 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating datasets...\n",
      "‚úì Train dataset: 57325 samples\n",
      "‚úì Validation dataset: 12284 samples\n",
      "‚úì Test dataset: 12284 samples\n",
      "\n",
      "üìù Tokenization example:\n",
      "Original text: 'Engaging RX Offers Superior Medical Reductions http://smallworldtho.spaces.live.com/default.aspx env'\n",
      "Token IDs shape: torch.Size([1, 50])\n",
      "First 10 token IDs: [101, 11973, 1054, 2595, 4107, 6020, 2966, 25006, 8299, 1024]\n",
      "Decoded tokens: ['[CLS]', 'engaging', 'r', '##x', 'offers', 'superior', 'medical', 'reductions', 'http', ':']\n",
      "\n",
      "üìä Token length statistics (sample):\n",
      "Mean tokens: 432.2\n",
      "95th percentile: 1228\n",
      "Texts truncated at 512: 226 (22.6%)\n",
      "\n",
      "üìä Token length statistics (sample):\n",
      "Mean tokens: 432.2\n",
      "95th percentile: 1228\n",
      "Texts truncated at 512: 226 (22.6%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize DistilBERT tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define maximum sequence length (DistilBERT max is 512)\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for email classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"üîÑ Creating datasets...\")\n",
    "train_dataset = EmailDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = EmailDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = EmailDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "print(f\"‚úì Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úì Validation dataset: {len(val_dataset)} samples\") \n",
    "print(f\"‚úì Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_text = X_train[0][:100]  # First 100 chars\n",
    "sample_encoding = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    max_length=50,  # Smaller for display\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Tokenization example:\")\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "print(f\"Token IDs shape: {sample_encoding['input_ids'].shape}\")\n",
    "print(f\"First 10 token IDs: {sample_encoding['input_ids'][0][:10].tolist()}\")\n",
    "print(f\"Decoded tokens: {tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][0][:10])}\")\n",
    "\n",
    "# Analyze text lengths after tokenization\n",
    "sample_lengths = []\n",
    "for text in X_train[:1000]:  # Sample first 1000 for speed\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors='pt')\n",
    "    sample_lengths.append(tokens['input_ids'].shape[1])\n",
    "\n",
    "print(f\"\\nüìä Token length statistics (sample):\")\n",
    "print(f\"Mean tokens: {np.mean(sample_lengths):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(sample_lengths, 95):.0f}\")\n",
    "print(f\"Texts truncated at {MAX_LENGTH}: {sum(1 for x in sample_lengths if x > MAX_LENGTH)} ({sum(1 for x in sample_lengths if x > MAX_LENGTH)/len(sample_lengths)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking accelerate version: 1.12.0\n",
      "‚úì accelerate version 1.12.0 is compatible with Trainer\n",
      "üîç Transformers version: 4.57.1\n",
      "‚úì All versions compatible for training\n"
     ]
    }
   ],
   "source": [
    "# Check accelerate version for Trainer compatibility\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"üîç Current accelerate version: {accelerate.__version__}\")\n",
    "    \n",
    "    # Try importing Trainer to see if it works\n",
    "    from transformers import Trainer\n",
    "    print(\"‚úì Trainer imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    if \"accelerate\" in str(e):\n",
    "        print(f\"‚ùå Accelerate import error: {str(e)}\")\n",
    "        print(\"üîß Trying to fix accelerate installation...\")\n",
    "        \n",
    "        # Try to install/upgrade accelerate\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"accelerate>=0.26.0\"])\n",
    "            print(\"‚úì Accelerate upgraded successfully!\")\n",
    "            \n",
    "            # Try importing again\n",
    "            import accelerate\n",
    "            from transformers import Trainer\n",
    "            print(f\"‚úì Now using accelerate version: {accelerate.__version__}\")\n",
    "            \n",
    "        except Exception as install_error:\n",
    "            print(f\"‚ùå Failed to fix accelerate: {str(install_error)}\")\n",
    "            print(\"Manual fix needed: pip install --upgrade 'accelerate>=0.26.0'\")\n",
    "    else:\n",
    "        print(f\"‚ùå Other import error: {str(e)}\")\n",
    "\n",
    "# Also check transformers version\n",
    "import transformers\n",
    "print(f\"üîç Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Try importing all required components\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    print(\"‚úì All training components imported successfully!\")\n",
    "    trainer_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Training components import failed: {str(e)}\")\n",
    "    trainer_available = False\n",
    "\n",
    "if trainer_available:\n",
    "    print(\"üöÄ Ready to proceed with training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training may not work. Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157c439",
   "metadata": {},
   "source": [
    "## 4. Create DistilBERT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff43302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading DistilBERT model...\n",
      "‚úì Model loaded: DistilBertForSequenceClassification\n",
      "‚úì Number of parameters: 66,955,010\n",
      "‚úì Trainable parameters: 66,955,010\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Trainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp.requires_grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models/distilbert-phishing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Changed from 500 to 400 (multiple of 200)\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_f1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# May help with memory issues\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Define metrics computation function\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(eval_pred):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khalil.nijaoui\\Documents\\perso\\projects\\ml_project\\venv\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     warnings.warn(\n\u001b[32m   1815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`torchdynamo` is deprecated and will be removed in version 5 of ü§ó Transformers. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1816\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_compile_backend` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1817\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1818\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khalil.nijaoui\\Documents\\perso\\projects\\ml_project\\venv\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2352\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2354\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:1001\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    999\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1003\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khalil.nijaoui\\Documents\\perso\\projects\\ml_project\\venv\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2228\u001b[39m         )\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2230\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "print(\"üîÑ Loading DistilBERT model...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification: phishing vs legitimate\n",
    "    output_attentions=True,  # Enable attention weights for explainability\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úì Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"‚úì Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"‚úì Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Try to use Trainer, fallback to manual training if it fails\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./models/distilbert-phishing\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=400,  # Changed from 500 to 400 (multiple of 200)\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=42,\n",
    "        dataloader_pin_memory=False,  # May help with memory issues\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # Define metrics computation function\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"‚úì Trainer initialized successfully!\")\n",
    "    print(f\"\\nüìä Training configuration:\")\n",
    "    print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  - Warmup steps: {training_args.warmup_steps}\")\n",
    "    print(f\"  - Weight decay: {training_args.weight_decay}\")\n",
    "    print(f\"  - Evaluation steps: {training_args.eval_steps}\")\n",
    "    print(f\"  - Save steps: {training_args.save_steps}\")\n",
    "    \n",
    "    use_trainer = True\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Trainer import failed: {str(e)}\")\n",
    "    print(\"üîÑ Setting up manual training instead...\")\n",
    "    \n",
    "    # Manual training setup\n",
    "    from torch.optim import AdamW\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "    \n",
    "    print(\"‚úì Manual training setup complete!\")\n",
    "    print(\"üìä Training will use manual PyTorch training loop\")\n",
    "    \n",
    "    use_trainer = False\n",
    "    \n",
    "    # Store training config for manual training\n",
    "    manual_training_config = {\n",
    "        'num_epochs': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 5e-5,\n",
    "        'weight_decay': 0.01\n",
    "    }\n",
    "\n",
    "print(f\"\\nüéØ Training method: {'Hugging Face Trainer' if use_trainer else 'Manual PyTorch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8c758",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pre-trained model exists\n",
    "model_save_path = Path(\"./models/distilbert-phishing\")\n",
    "if model_save_path.exists():\n",
    "    print(\"üîÑ Loading pre-trained model...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_save_path,\n",
    "        output_attentions=True\n",
    "    ).to(device)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_save_path)\n",
    "    print(\"‚úì Pre-trained model loaded successfully!\")\n",
    "else:\n",
    "    print(\"üöÄ Starting model training...\")\n",
    "    print(\"This may take 10-30 minutes depending on your hardware...\")\n",
    "    \n",
    "    if use_trainer:\n",
    "        # Use Hugging Face Trainer\n",
    "        try:\n",
    "            trainer.train()\n",
    "            print(\"‚úì Training completed successfully!\")\n",
    "            \n",
    "            # Save the best model\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "            print(f\"‚úì Model saved to {training_args.output_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Trainer failed: {str(e)}\")\n",
    "            print(\"üîÑ Switching to manual training...\")\n",
    "            use_trainer = False\n",
    "    \n",
    "    if not use_trainer:\n",
    "        # Manual training loop\n",
    "        print(\"üìö Starting manual training...\")\n",
    "        \n",
    "        model.train()\n",
    "        total_steps = len(train_dataloader) * manual_training_config['num_epochs']\n",
    "        \n",
    "        for epoch in range(manual_training_config['num_epochs']):\n",
    "            print(f\"\\nüîÑ Epoch {epoch + 1}/{manual_training_config['num_epochs']}\")\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Print progress\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"  Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, \n",
    "                                  attention_mask=attention_mask, \n",
    "                                  labels=labels)\n",
    "                    val_loss += outputs.loss.item()\n",
    "                    \n",
    "                    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                    val_correct += (predictions == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "            \n",
    "            val_accuracy = val_correct / val_total\n",
    "            print(f\"  Validation Loss: {val_loss/len(val_dataloader):.4f}\")\n",
    "            print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Save manually trained model\n",
    "        model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"‚úì Manual training completed! Model saved to {model_save_path}\")\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_save_path,\n",
    "    output_attentions=True\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nüéØ Model training summary:\")\n",
    "print(f\"‚úì Model successfully trained and loaded\")\n",
    "print(f\"‚úì Model location: {model_save_path}\")\n",
    "print(f\"‚úì Ready for evaluation and explainability analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07490c4b",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation pipeline\n",
    "eval_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to predict with probabilities\n",
    "def predict_text(text):\n",
    "    \"\"\"Predict class and probabilities for a given text\"\"\"\n",
    "    results = eval_pipeline(text)\n",
    "    probs = {result['label']: result['score'] for result in results}\n",
    "    predicted_class = max(probs.keys(), key=lambda x: probs[x])\n",
    "    return predicted_class, probs\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"üîÑ Evaluating model on test set...\")\n",
    "test_predictions = []\n",
    "test_probabilities = []\n",
    "\n",
    "# Batch prediction for efficiency\n",
    "batch_size = 32\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    batch_texts = X_test[i:i + batch_size]\n",
    "    batch_results = eval_pipeline(batch_texts)\n",
    "    \n",
    "    for result in batch_results:\n",
    "        # Convert to binary classification format\n",
    "        if isinstance(result, list):  # Multiple labels returned\n",
    "            phishing_score = next(r['score'] for r in result if r['label'] == 'LABEL_1')\n",
    "            prediction = 1 if phishing_score > 0.5 else 0\n",
    "        else:  # Single label returned\n",
    "            prediction = 1 if result['label'] == 'LABEL_1' else 0\n",
    "            phishing_score = result['score'] if result['label'] == 'LABEL_1' else 1 - result['score']\n",
    "        \n",
    "        test_predictions.append(prediction)\n",
    "        test_probabilities.append(phishing_score)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_pred = np.array(test_predictions)\n",
    "y_prob = np.array(test_probabilities)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate PR-AUC\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Phishing']))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Legitimate', 'Phishing'],\n",
    "           yticklabels=['Legitimate', 'Phishing'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)\n",
    "plt.plot(recall_curve, precision_curve, marker='.', label=f'PR-AUC = {pr_auc:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample predictions with confidence scores\n",
    "print(f\"\\nüîç Sample Predictions:\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    text = X_test[i][:100] + \"...\" if len(X_test[i]) > 100 else X_test[i]\n",
    "    true_label = \"Phishing\" if y_test[i] == 1 else \"Legitimate\"\n",
    "    pred_label = \"Phishing\" if y_pred[i] == 1 else \"Legitimate\"\n",
    "    confidence = y_prob[i] if y_pred[i] == 1 else 1 - y_prob[i]\n",
    "    \n",
    "    status = \"‚úì\" if y_test[i] == y_pred[i] else \"‚úó\"\n",
    "    print(f\"{status} True: {true_label:10} | Pred: {pred_label:10} | Conf: {confidence:.3f}\")\n",
    "    print(f\"  Text: '{text}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3eedcb",
   "metadata": {},
   "source": [
    "## 7. Model Explainability with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create LIME explainer\n",
    "    explainer = LimeTextExplainer(class_names=['Legitimate', 'Phishing'])\n",
    "\n",
    "    # Wrapper function for LIME (needs to return probabilities)\n",
    "    def predict_proba_wrapper(texts):\n",
    "        \"\"\"Wrapper function that returns probabilities for LIME\"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                results = eval_pipeline(text)\n",
    "                if isinstance(results, list):\n",
    "                    # Extract probabilities for both classes\n",
    "                    probs = [0.0, 0.0]  # [legitimate, phishing]\n",
    "                    for result in results:\n",
    "                        if result['label'] == 'LABEL_0':  # Legitimate\n",
    "                            probs[0] = result['score']\n",
    "                        elif result['label'] == 'LABEL_1':  # Phishing\n",
    "                            probs[1] = result['score']\n",
    "                    probabilities.append(probs)\n",
    "                else:\n",
    "                    # Single result case\n",
    "                    if results['label'] == 'LABEL_0':\n",
    "                        probabilities.append([results['score'], 1 - results['score']])\n",
    "                    else:\n",
    "                        probabilities.append([1 - results['score'], results['score']])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {str(e)}\")\n",
    "                probabilities.append([0.5, 0.5])  # Default to uncertain\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "    # Select sample emails for explanation\n",
    "    # Pick one phishing and one legitimate email\n",
    "    phishing_idx = next(i for i, label in enumerate(y_test) if label == 1)\n",
    "    legitimate_idx = next(i for i, label in enumerate(y_test) if label == 0)\n",
    "\n",
    "    sample_emails = [\n",
    "        (X_test[phishing_idx], \"Phishing\", phishing_idx),\n",
    "        (X_test[legitimate_idx], \"Legitimate\", legitimate_idx)\n",
    "    ]\n",
    "\n",
    "    print(\"üîç LIME Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for email_text, email_type, idx in sample_emails:\n",
    "        print(f\"\\nüìß Analyzing {email_type} Email (Index: {idx}):\")\n",
    "        print(f\"Text preview: '{email_text[:200]}...'\")\n",
    "        \n",
    "        # Generate explanation\n",
    "        try:\n",
    "            explanation = explainer.explain_instance(\n",
    "                email_text,\n",
    "                predict_proba_wrapper,\n",
    "                num_features=20,\n",
    "                num_samples=500\n",
    "            )\n",
    "            \n",
    "            # Get the explanation for the predicted class\n",
    "            predicted_class = explanation.predict_proba[1]  # Phishing probability\n",
    "            actual_class = y_test[idx]\n",
    "            \n",
    "            print(f\"Prediction: {predicted_class:.3f} (Phishing probability)\")\n",
    "            print(f\"Actual: {'Phishing' if actual_class == 1 else 'Legitimate'}\")\n",
    "            \n",
    "            # Show top influential words\n",
    "            print(f\"\\nüîç Top words influencing prediction:\")\n",
    "            explanation_list = explanation.as_list()\n",
    "            \n",
    "            for word, importance in explanation_list[:10]:\n",
    "                direction = \"‚Üí Phishing\" if importance > 0 else \"‚Üí Legitimate\"\n",
    "                print(f\"  '{word}': {importance:+.3f} {direction}\")\n",
    "            \n",
    "            # Save explanation as HTML (optional)\n",
    "            html_path = f\"lime_explanation_{email_type.lower()}_{idx}.html\"\n",
    "            explanation.save_to_file(html_path)\n",
    "            print(f\"\\nüìÑ Detailed explanation saved to: {html_path}\")\n",
    "            \n",
    "            # Show explanation in notebook\n",
    "            explanation.show_in_notebook(text=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating LIME explanation: {str(e)}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå LIME not available. Please install with: pip install lime\")\n",
    "    print(\"Skipping LIME analysis...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in LIME analysis: {str(e)}\")\n",
    "    print(\"This might be due to model compatibility issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a38057",
   "metadata": {},
   "source": [
    "## 8. Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3af0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"üîÑ Setting up SHAP explainer...\")\n",
    "    \n",
    "    # Create a wrapper function for SHAP\n",
    "    def model_predict(texts):\n",
    "        \"\"\"Wrapper for SHAP that returns probabilities\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        probabilities = []\n",
    "        for text in texts:\n",
    "            results = eval_pipeline(text)\n",
    "            if isinstance(results, list):\n",
    "                # Get phishing probability\n",
    "                phishing_prob = next((r['score'] for r in results if r['label'] == 'LABEL_1'), 0.5)\n",
    "            else:\n",
    "                phishing_prob = results['score'] if results['label'] == 'LABEL_1' else 1 - results['score']\n",
    "            probabilities.append([1 - phishing_prob, phishing_prob])\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "    \n",
    "    # Initialize SHAP explainer\n",
    "    # Using a sample of training data as background dataset\n",
    "    background_texts = X_train[:100]  # Use subset for efficiency\n",
    "    \n",
    "    explainer = shap.Explainer(model_predict, background_texts)\n",
    "    \n",
    "    # Select sample texts for SHAP analysis\n",
    "    sample_texts = []\n",
    "    sample_labels = []\n",
    "    \n",
    "    # Get one example from each class\n",
    "    for label in [0, 1]:\n",
    "        idx = next(i for i, l in enumerate(y_test) if l == label)\n",
    "        sample_texts.append(X_test[idx][:500])  # Truncate for efficiency\n",
    "        sample_labels.append(label)\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(sample_texts)} sample emails with SHAP...\")\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    try:\n",
    "        shap_values = explainer(sample_texts)\n",
    "        \n",
    "        print(\"‚úì SHAP values computed successfully!\")\n",
    "        \n",
    "        # Display results for each sample\n",
    "        for i, (text, true_label) in enumerate(zip(sample_texts, sample_labels)):\n",
    "            print(f\"\\nüìß Sample {i+1} ({'Phishing' if true_label == 1 else 'Legitimate'}):\")\n",
    "            print(f\"Text: '{text[:150]}...'\")\n",
    "            \n",
    "            # Get SHAP values for phishing class (class 1)\n",
    "            shap_vals = shap_values[i].values[:, 1]  # Phishing class\n",
    "            \n",
    "            # Get the words (features)\n",
    "            words = shap_values[i].data\n",
    "            \n",
    "            # Find top positive and negative contributions\n",
    "            word_importance = list(zip(words, shap_vals))\n",
    "            word_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(f\"üîç Top contributing words:\")\n",
    "            for word, importance in word_importance[:10]:\n",
    "                direction = \"‚Üí Phishing\" if importance > 0 else \"‚Üí Legitimate\"\n",
    "                print(f\"  '{word}': {importance:+.4f} {direction}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # SHAP summary plot\n",
    "        try:\n",
    "            plt.subplot(2, 1, 1)\n",
    "            shap.summary_plot(shap_values[:, :, 1], feature_names=sample_texts, show=False)\n",
    "            plt.title(\"SHAP Summary Plot - Feature Importance for Phishing Classification\")\n",
    "            \n",
    "            # SHAP waterfall plot for first sample\n",
    "            plt.subplot(2, 1, 2)\n",
    "            shap.waterfall_plot(shap_values[0, :, 1], show=False)\n",
    "            plt.title(f\"SHAP Waterfall Plot - Sample 1 ({'Phishing' if sample_labels[0] == 1 else 'Legitimate'})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not create SHAP plots: {str(e)}\")\n",
    "            print(\"This might be due to visualization compatibility issues.\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error computing SHAP values: {str(e)}\")\n",
    "        print(\"SHAP analysis may require more memory or different configuration.\")\n",
    "        \n",
    "        # Fallback: Simple feature importance analysis\n",
    "        print(\"\\nüîÑ Performing simplified feature analysis...\")\n",
    "        \n",
    "        for i, text in enumerate(sample_texts):\n",
    "            words = text.split()[:50]  # First 50 words\n",
    "            print(f\"\\nüìß Sample {i+1} word analysis:\")\n",
    "            \n",
    "            # Simple word-by-word importance\n",
    "            base_pred = model_predict([text])[0][1]  # Phishing probability\n",
    "            \n",
    "            important_words = []\n",
    "            for word in words:\n",
    "                # Remove word and see change in prediction\n",
    "                modified_text = text.replace(word, '')\n",
    "                if modified_text != text:\n",
    "                    modified_pred = model_predict([modified_text])[0][1]\n",
    "                    importance = base_pred - modified_pred\n",
    "                    important_words.append((word, importance))\n",
    "            \n",
    "            # Sort by importance\n",
    "            important_words.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(\"üîç Most influential words:\")\n",
    "            for word, importance in important_words[:10]:\n",
    "                direction = \"‚Üí Phishing\" if importance > 0 else \"‚Üí Legitimate\"\n",
    "                print(f\"  '{word}': {importance:+.4f} {direction}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå SHAP not available. Please install with: pip install shap\")\n",
    "    print(\"Skipping SHAP analysis...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in SHAP analysis: {str(e)}\")\n",
    "    print(\"SHAP analysis might require additional configuration or memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c694a",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Visualization and Feature Analysis\n",
    "\n",
    "def get_attention_weights(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Extract attention weights from the model\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get model outputs including attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    attention = outputs.attentions  # Tuple of attention matrices\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    return attention, tokens, outputs.logits\n",
    "\n",
    "def visualize_attention(text, model, tokenizer, layer=5, head=0):\n",
    "    \"\"\"Visualize attention weights for a specific layer and head\"\"\"\n",
    "    attention, tokens, logits = get_attention_weights(text, model, tokenizer)\n",
    "    \n",
    "    # Get attention for specified layer and head\n",
    "    att_matrix = attention[layer][0][head].cpu().numpy()\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Filter out special tokens for better visualization\n",
    "    valid_tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    valid_indices = [i for i, token in enumerate(tokens) if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    \n",
    "    if len(valid_indices) > 30:  # Limit for readability\n",
    "        valid_indices = valid_indices[:30]\n",
    "        valid_tokens = valid_tokens[:30]\n",
    "    \n",
    "    # Extract relevant attention matrix\n",
    "    att_subset = att_matrix[np.ix_(valid_indices, valid_indices)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.heatmap(att_subset, \n",
    "                xticklabels=valid_tokens,\n",
    "                yticklabels=valid_tokens,\n",
    "                cmap='Blues',\n",
    "                cbar=True)\n",
    "    plt.title(f'Attention Weights - Layer {layer}, Head {head}')\n",
    "    plt.xlabel('Tokens (Attending To)')\n",
    "    plt.ylabel('Tokens (Attending From)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Plot prediction probabilities\n",
    "    plt.subplot(2, 1, 2)\n",
    "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    classes = ['Legitimate', 'Phishing']\n",
    "    colors = ['green' if probs[0] > probs[1] else 'red']\n",
    "    \n",
    "    bars = plt.bar(classes, probs, color=['lightblue', 'lightcoral'])\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Model Prediction Probabilities')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, prob in zip(bars, probs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{prob:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return att_matrix, tokens, probs\n",
    "\n",
    "# Analyze attention for sample emails\n",
    "print(\"üîç Attention Weight Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select examples for attention analysis\n",
    "examples = [\n",
    "    (X_test[phishing_idx], \"Phishing Email\"),\n",
    "    (X_test[legitimate_idx], \"Legitimate Email\")\n",
    "]\n",
    "\n",
    "for text, label in examples:\n",
    "    print(f\"\\nüìß Analyzing: {label}\")\n",
    "    print(f\"Text preview: '{text[:150]}...'\")\n",
    "    \n",
    "    try:\n",
    "        # Visualize attention\n",
    "        att_matrix, tokens, probs = visualize_attention(text, model, tokenizer)\n",
    "        plt.suptitle(f'Attention Analysis - {label}', fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Find most attended tokens\n",
    "        avg_attention = np.mean(att_matrix, axis=0)  # Average attention received by each token\n",
    "        token_attention = list(zip(tokens, avg_attention))\n",
    "        token_attention.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"üéØ Most attended tokens:\")\n",
    "        for token, attention in token_attention[:10]:\n",
    "            if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                print(f\"  '{token}': {attention:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in attention analysis: {str(e)}\")\n",
    "\n",
    "# Word frequency analysis in phishing vs legitimate emails\n",
    "print(\"\\nüìä Word Frequency Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Collect all words from each class\n",
    "phishing_words = []\n",
    "legitimate_words = []\n",
    "\n",
    "for text, label in zip(X_train, y_train):\n",
    "    words = text.lower().split()\n",
    "    if label == 1:\n",
    "        phishing_words.extend(words)\n",
    "    else:\n",
    "        legitimate_words.extend(words)\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import Counter\n",
    "phishing_counter = Counter(phishing_words)\n",
    "legitimate_counter = Counter(legitimate_words)\n",
    "\n",
    "# Find distinctive words\n",
    "common_phishing = phishing_counter.most_common(20)\n",
    "common_legitimate = legitimate_counter.most_common(20)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "words, counts = zip(*common_phishing[:15])\n",
    "plt.barh(range(len(words)), counts, color='red', alpha=0.7)\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Most Common Words in Phishing Emails')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "words, counts = zip(*common_legitimate[:15])\n",
    "plt.barh(range(len(words)), counts, color='green', alpha=0.7)\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Most Common Words in Legitimate Emails')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Top distinctive phishing keywords:\")\n",
    "for word, count in common_phishing[:10]:\n",
    "    if len(word) > 3:  # Filter out short words\n",
    "        print(f\"  '{word}': {count} occurrences\")\n",
    "\n",
    "print(\"\\nüîç Top legitimate keywords:\")\n",
    "for word, count in common_legitimate[:10]:\n",
    "    if len(word) > 3:\n",
    "        print(f\"  '{word}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abec6c3",
   "metadata": {},
   "source": [
    "## 10. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab49b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, tokenizer, and results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üíæ Saving model and results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer (if not already saved)\n",
    "model_dir = Path(\"models/distilbert-phishing\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    print(f\"‚úì Model saved to {model_dir}\")\n",
    "else:\n",
    "    print(f\"‚úì Model already exists at {model_dir}\")\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    \"model_info\": {\n",
    "        \"model_name\": model_name,\n",
    "        \"architecture\": \"DistilBERT for sequence classification\",\n",
    "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"training_date\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(X_train),\n",
    "        \"val_size\": len(X_val), \n",
    "        \"test_size\": len(X_test),\n",
    "        \"class_distribution\": {\n",
    "            \"legitimate\": int(sum(1 for x in y_train if x == 0)),\n",
    "            \"phishing\": int(sum(1 for x in y_train if x == 1))\n",
    "        }\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_precision\": float(precision),\n",
    "        \"test_recall\": float(recall),\n",
    "        \"test_f1_score\": float(f1),\n",
    "        \"test_pr_auc\": float(pr_auc)\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = results_dir / \"distilbert_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to {results_file}\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'phishing_probability': y_prob,\n",
    "    'correct_prediction': y_test == y_pred\n",
    "})\n",
    "\n",
    "predictions_file = results_dir / \"distilbert_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"‚úì Predictions saved to {predictions_file}\")\n",
    "\n",
    "# Create a simple inference function for future use\n",
    "def classify_email(text, model_path=\"models/distilbert-phishing\"):\n",
    "    \"\"\"\n",
    "    Classify a single email text using the trained DistilBERT model\n",
    "    \n",
    "    Args:\n",
    "        text (str): Email text to classify\n",
    "        model_path (str): Path to the saved model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Classification results with prediction and confidence\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Create pipeline\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    results = classifier(text)\n",
    "    \n",
    "    # Format results\n",
    "    phishing_score = next(r['score'] for r in results if r['label'] == 'LABEL_1')\n",
    "    prediction = \"Phishing\" if phishing_score > 0.5 else \"Legitimate\"\n",
    "    confidence = phishing_score if prediction == \"Phishing\" else 1 - phishing_score\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"confidence\": confidence,\n",
    "        \"phishing_probability\": phishing_score,\n",
    "        \"legitimate_probability\": 1 - phishing_score\n",
    "    }\n",
    "\n",
    "# Save the inference function\n",
    "import pickle\n",
    "with open(results_dir / \"classify_email_function.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classify_email, f)\n",
    "\n",
    "print(f\"‚úì Inference function saved to {results_dir / 'classify_email_function.pkl'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüéâ Analysis Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Model Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1:.4f}\")\n",
    "print(f\"  ‚Ä¢ PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"  ‚Ä¢ Model: {model_dir}\")\n",
    "print(f\"  ‚Ä¢ Results: {results_file}\")\n",
    "print(f\"  ‚Ä¢ Predictions: {predictions_file}\")\n",
    "print(f\"\\nüîç Explainability Features:\")\n",
    "print(f\"  ‚Ä¢ LIME explanations for individual predictions\")\n",
    "print(f\"  ‚Ä¢ SHAP values for feature importance\")\n",
    "print(f\"  ‚Ä¢ Attention weight visualizations\")\n",
    "print(f\"  ‚Ä¢ Word frequency analysis\")\n",
    "\n",
    "# Example usage of the saved model\n",
    "print(f\"\\nüß™ Testing saved model:\")\n",
    "sample_text = \"Get 50% off Viagra! Click here now for amazing deals!\"\n",
    "result = classify_email(sample_text)\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Prediction: {result['prediction']} (confidence: {result['confidence']:.3f})\")\n",
    "print(f\"\\n‚úÖ DistilBERT explainable phishing detection model is ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
